@article{ackoff1989,
  title = {From data to wisdom},
  author = {Ackoff, Russell},
  date = {1989},
  journaltitle = {Journal of Applied Systems Analysis},
  volume = {16},
  pages = {3--9},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science}
}

@software{allaire,
  title = {Quarto},
  author = {Allaire, J. J. and Teague, Charles and Xie, Yihui and Dervieux, Christophe},
  doi = {10.5281/ZENODO.5960048},
  abstract = {Quarto is an open-source scientific and technical publishing system built on Pandoc.},
  organization = {Zenodo},
  keywords = {computer science,dynamic documents,exact sciences,pandoc,r (programming language)}
}

@article{bernardes2009a,
  title = {Ensino e aprendizagem como unidade dialética na atividade pedagógica},
  author = {Bernardes, Maria Eliza Mattosinho},
  date = {2009-12},
  journaltitle = {Psicologia Escolar e Educacional},
  shortjournal = {Psicol. Esc. Educ.},
  volume = {13},
  number = {2},
  pages = {235--242},
  issn = {1413-8557},
  doi = {10.1590/S1413-85572009000200005},
  abstract = {Considerar a atividade pedagógica como unidade dialética requer que sejam investigadas as condições e circunstâncias que tornam possível tal objetivação e as especificidades da mesma a partir do materialismo histórico dialético, da psicologia histórico-cultural e da teoria da atividade. Concebe-se que a correspondência entre motivos/necessidades, objetivos, ações e operações nas atividades principais dos estudantes e dos educadores cria situações favoráveis para que ocorra a aprendizagem consciente. Tal fato é dependente da consciência da função ontológica do educador e do estudante como sujeitos ativos no processo de ensino-aprendizagem. Considera-se ser possível conceber a unidade dialética desde que a mesma represente uma das condições para a superação das características do sistema educacional vigente, por caracterizar-se a atividade material do homem como a que transforma o mundo material e social em mundo humano.},
  langid = {portuguese},
  keywords = {education,human sciences,preparação pedagógica},
  note = {Teaching and learning as a dialectical unity in pedagogical activity}
}

@incollection{box1979,
  title = {Robustness in the strategy of scientific model building},
  booktitle = {Robustness in statistics},
  author = {Box, George E. P.},
  editor = {Launer, Robert L. and Wilkinson, Graham N.},
  date = {1979},
  pages = {201--236},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-438150-6.50018-2},
  abstract = {Robustness may be defined as the property of a procedure which renders the answers it gives insensitive to departures, of a kind which occur in practice, from ideal assumptions. Since assumptions imply some kind of scientific model, I believe that it is necessary to look at the process of scientific modelling itself to understand the nature of and the need for robust procedures. Against such a view it might be urged that some useful robust procedures have been derived empirically without an explicitly stated model. However, an empirical procedure implies some unstated model and there is often great virtue in bringing into the open the kind of assumptions that lead to useful methods. The need for robust methods seems to be intimately mixed up with the need for simple models. This we now discuss.},
  isbn = {978-0-12-438150-6},
  langid = {english},
  keywords = {exact sciences,modeling,probability and statistics,science}
}

@book{boyer2011,
  title = {A history of mathematics},
  author = {Boyer, Carl B. and Merzbach, Uta C.},
  namea = {Asimov, Isaac},
  nameatype = {collaborator},
  date = {2011},
  origdate = {1968},
  edition = {3},
  publisher = {John Wiley \& Sons},
  location = {Hoboken, N.J.},
  isbn = {978-0-470-52548-7},
  langid = {english},
  keywords = {exact sciences,history,mathematics}
}

@online{broman2013,
  title = {Data science is statistics},
  author = {Broman, Karl},
  date = {2013-04-05T13:12:35+00:00},
  url = {https://kbroman.wordpress.com/2013/04/05/data-science-is-statistics},
  urldate = {2024-06-10},
  langid = {english},
  organization = {The stupidest thing...},
  keywords = {data science,exact sciences,probability and statistics}
}

@book{brookshear2020,
  title = {Computer science: {{An}} overview},
  shorttitle = {Computer science},
  author = {Brookshear, J. Glenn and Brylow, Dennis},
  date = {2020},
  edition = {13 ed., Global edition},
  publisher = {Pearson},
  location = {Harlow, UK},
  abstract = {Computer Science: An Overview, 13th edition, Global Edition, by J. Glenn Brookshear, and Dennis Brylow, is written for students from all backgrounds, giving you a bottom-up, concrete-to-abstract foundation in the subject. Its broad coverage encourages a practical and realistic understanding of computer science, covering all the major concepts. The book's broad background exposes beginning computer science students to the breadth of the subject they plan to major in and teaches students from other backgrounds how to relate to the technical society in which they live. Learn in a flexible way with independent chapters you can study in any order with full-colour design to help you engage with the information. The text also uses Python to provide programming tools for exploration and experimentation in your learning. This 13th edition has been corrected and updated in each chapter to refine your learning experience. With more than 1,000 questions and exercises, the book trains your thinking skills with useful chapter review problems and contains questions surrounding social issues to reinforce core concepts. This text is comprehensive and highly accessible, making it ideal for undergraduate studies in computer science.},
  isbn = {978-1-292-26344-1},
  langid = {english},
  pagetotal = {735},
  keywords = {computer science,exact sciences,fundamentals of computer science,textbooks}
}

@book{bryan,
  title = {Happy {{Git}} and {{GitHub}} for the {{useR}}},
  author = {Bryan, Jennifer and Hester, Jim},
  series = {{{STAT}} 545 {{TAs}}},
  url = {https://happygitwithr.com},
  abstract = {Happy Git provides opinionated instructions on how to: - Install Git and get it working smoothly with GitHub, in the shell and in the RStudio IDE. - Develop a few key workflows that cover your most common tasks. - Integrate Git and GitHub into your daily work with R and R Markdown. The target reader is someone who uses R for data analysis or who works on R packages, although some of the content may be useful to those working in adjacent areas.},
  langid = {american},
  keywords = {computer science,r (programming language),r (programming language) fundamentals}
}

@book{bryana,
  title = {What they forgot to teach you about {{R}}: the stuff you need to know about {{R}}, besides data analysis},
  shorttitle = {What they forgot to teach you about {{R}}},
  author = {Bryan, Jennifer and Hester, Jim and Pileggi, Shannon and Aja, E. David},
  url = {https://rstats.wtf},
  abstract = {This book focuses on content intrinsically related to the infrastructure surrounding data analysis in R, but does not delve into the data analysis itself. - *A holistic workflow* provides guidance on project-oriented workflows that address common sources of friction in data analysis. - *Personal R administration* empowers R users to confidently manage their R programming environment. - *All is Fail* showcases functions, options, and RStudio capabilities for debugging code, facilitating more efficient resolution of errant code.},
  langid = {american},
  keywords = {computer science,r (programming language),r (programming language) fundamentals}
}

@article{cao2017,
  title = {Data science: a comprehensive overview},
  shorttitle = {Data science},
  author = {Cao, Longbing},
  date = {2017-06-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {3},
  pages = {43},
  issn = {0360-0300},
  doi = {10.1145/3076253},
  abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics}
}

@article{carvalho2021a,
  title = {Measuring sustainable food systems in {{Brazil}}: a framework and multidimensional index to evaluate socioeconomic, nutritional, and environmental aspects},
  shorttitle = {Measuring sustainable food systems in {{Brazil}}},
  author = {family=Carvalho, given=Aline Martins, prefix=de, useprefix=false and Verly Jr, Eliseu and Marchioni, Dirce Maria and Jones, Andrew D.},
  date = {2021-07-01},
  journaltitle = {World Development},
  shortjournal = {World Development},
  volume = {143},
  pages = {105470},
  issn = {0305-750X},
  doi = {10.1016/j.worlddev.2021.105470},
  abstract = {The sustainability of food systems is commonly measured at the global or national level using multidimensional indices. However, it is not possible to use such indices at subnational levels, because the data for many indicators are not available and large countries such as Brazil have completely different food systems depending on the state or region considered. This paper presents a sustainable food systems framework for Brazil and the Brazilian Multidimensional Index for Sustainable Food Systems (MISFS), which is the first index of its kind that considers local behaviors and actions to rank states and regions of the country. The dimensions and indicators included in the index were identified from a literature review and an exclusion criteria, which considered the Brazilian context, that ended up with 17 indicators divided in three dimensions (environmental, socioeconomic, and nutritional). We followed well-known procedures to create an interpretable multidimensional index, which include normalization, weighting, aggregating, and sensitivity analysis. The performance of the states was starkly different, regarding the indicators and dimensions considered. No state presented a high score for all three dimensions, and even states with a high MISFS score had room to improve in one or two dimensions. In general, high-income states scored well on socioeconomic indicators (e.g., agriculture education), but poorly on most environmental indicators (e.g., water footprint), and some nutritional indicators (e.g., obesity rate). This index is the first initiative to measure the Brazilian food systems according to local practices and behaviors at state level and it is a starting point to help policymakers understand the opportunities of improvement of their food systems. This study may also help other countries to develop their own frameworks and strategies to improve their regionalized food systems.},
  keywords = {brazil,environmental sciences,exact sciences,health sciences,índice,misfs,nutrition}
}

@book{casella2002,
  title = {Statistical inference},
  author = {Casella, George and Berger, Roger L.},
  date = {2002},
  series = {Duxbury advanced series},
  edition = {2},
  publisher = {Duxbury},
  location = {Pacific Grove, CA},
  isbn = {0-534-24312-6},
  langid = {english},
  pagetotal = {660},
  keywords = {exact sciences,fundamentals of probability and statistics,probability and statistics,statistical inference,textbooks,very normal recommendations}
}

@book{chang2018,
  title = {R graphics cookbook: {{Practical}} recipes for visualizing data},
  shorttitle = {R graphics cookbook},
  author = {Chang, Winston},
  date = {2018-12-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Beijing Boston Farnham},
  url = {https://r-graphics.org},
  abstract = {This O’Reilly cookbook provides more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly―without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.  Most of the recipes in this second edition use the updated version of the ggplot2 package, a powerful and flexible way to make graphs in R. You’ll also find expanded content about the visual design of graphics. If you have at least a basic understanding of the R language, you’re ready to get started with this easy-to-use reference. Use R’s default graphics for quick exploration of data Create a variety of bar graphs, line graphs, and scatter plots Summarize data distributions with histograms, density curves, box plots, and more Provide annotations to help viewers interpret data Control the overall appearance of graphics Explore options for using colors in plots Create network graphs, heat maps, and 3D scatter plots Get your data into shape using packages from the tidyverse},
  isbn = {978-1-4919-7860-3},
  langid = {english},
  pagetotal = {441},
  keywords = {computer science,data science,data visualization,design,exact sciences,ggplot2,grammar of graphics,r (programming language),textbooks}
}

@video{code.org2018,
  entrysubtype = {video},
  title = {How computers work},
  editor = {{Code.org}},
  editortype = {director},
  date = {2018-01-30},
  url = {https://youtube.com/playlist?list=PLzdnOPI1iJNcsRwJhvksEo1tJqjIqWbN-&si=WkuM8c-AKI-NZ3td},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {computer science,fundamentals of computer science}
}

@book{cohen1988a,
  title = {Statistical power analysis for the behavioral sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2},
  publisher = {Lawrence Erlbaum Associates},
  location = {Hillsdale, NJ},
  abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes: - A chapter covering power analysis in set correlation and multivariate methods; - A chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; - Expanded power and sample size tables for multiple regression/correlation.},
  isbn = {978-0-8058-0283-2},
  langid = {english},
  pagetotal = {567},
  keywords = {effect size,exact sciences,power analysis,probability and statistics,sampling,textbooks}
}

@book{coronel2019,
  title = {Database systems: design, implementation, and management},
  shorttitle = {Database systems},
  author = {Coronel, Carlos and Morris, Steven A.},
  date = {2019},
  edition = {13},
  publisher = {Cengage},
  location = {Boston, MA},
  isbn = {978-1-337-62790-0},
  langid = {english},
  pagetotal = {802},
  keywords = {computer science,databases,exact sciences,information science,modeling,relational databases}
}

@article{couch2021,
  title = {infer: {{An R}} package for tidyverse-friendly statistical inference},
  shorttitle = {infer},
  author = {Couch, Simon and Bray, Andrew and Ismay, Chester and Chasnovski, Evgeni and Baumer, Benjamin and Çetinkaya-Rundel, Mine},
  date = {2021-09-16},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {6},
  number = {65},
  pages = {3661},
  issn = {2475-9066},
  doi = {10.21105/joss.03661}
}

@book{degroot2012a,
  title = {Probability and statistics},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  date = {2012},
  edition = {4},
  publisher = {Addison-Wesley},
  location = {Boston},
  isbn = {978-0-321-50046-5},
  langid = {english},
  pagetotal = {893},
  keywords = {exact sciences,fundamentals of probability and statistics,probability and statistics,textbooks},
  annotation = {OCLC: ocn502674206}
}

@article{denning2005,
  title = {Is computer science science?},
  author = {Denning, Peter J.},
  date = {2005-04-01},
  journaltitle = {Communications of the ACM},
  volume = {48},
  number = {4},
  pages = {27--31},
  issn = {0001-0782},
  doi = {10.1145/1053291.1053309},
  abstract = {Computer science meets every criterion for being a science, but it has a self-inflicted credibility problem.},
  langid = {english},
  keywords = {computer science,exact sciences,perspectives}
}

@article{dhar2023,
  title = {Data science and prediction},
  author = {Dhar, Vasant},
  date = {2023-12-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {64--73},
  issn = {0001-0782},
  doi = {10.1145/2500499},
  abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics}
}

@book{ellis2010,
  title = {The essential guide to effect sizes: statistical power, meta-analysis, and the interpretation of research results},
  shorttitle = {The essential guide to effect sizes},
  editor = {Ellis, Paul D.},
  date = {2010},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK},
  abstract = {This succinct and jargon-free introduction to effect sizes gives students and researchers the tools they need to interpret the practical significance of their results. Using a class-tested approach that includes numerous examples and step-by-step exercises, it introduces and explains three of the most important issues relating to the practical significance of research results: the reporting and interpretation of effect sizes (Part I), the analysis of statistical power (Part II), and the meta-analytic pooling of effect size estimates drawn from different studies (Part III). The book concludes with a handy list of recommendations for those actively engaged in or currently preparing research projects.},
  isbn = {978-0-521-19423-5},
  langid = {english},
  pagetotal = {173},
  keywords = {effect size,exact sciences,probability and statistics,textbooks}
}

@article{ellis2018,
  title = {How to share data for collaboration},
  author = {Ellis, Shannon E. and Leek, Jeffrey T.},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  eprint = {32981941},
  eprinttype = {pubmed},
  pages = {53--57},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375987},
  abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
  keywords = {data science,exact sciences,interdisciplinary fields,probability and statistics,tidy data}
}

@book{engels2020a,
  title = {Dialética da natureza},
  author = {Engels, Friedrich},
  translator = {Schneider, Nélio},
  date = {2020},
  origdate = {1873},
  publisher = {Boitempo},
  location = {São Paulo},
  isbn = {978-65-5717-024-3},
  langid = {brazilian},
  origlanguage = {de},
  keywords = {biological sciences,complexity science,dialectical materialism,dialectics,human sciences,interdisciplinary fields,marxism,materialism,sociology},
  note = {Dialektik der natur}
}

@book{freire2011,
  title = {Pedagogia da autonomia: Saberes necessários à prática educativa},
  shorttitle = {Pedagogia Da Autonomia},
  author = {Freire, Paulo},
  date = {2011},
  publisher = {Paz e Terra},
  location = {São Paulo},
  isbn = {978-85-7753-226-1},
  langid = {portuguese},
  keywords = {education,human sciences,paulo freire,preparação pedagógica},
  note = {Pedagogy of autonomy: Necessary knowledge for educational practice}
}

@book{frey2022,
  title = {The {{SAGE}} encyclopedia of research design},
  editor = {Frey, Bruce B.},
  date = {2022},
  edition = {2},
  publisher = {SAGE Publications},
  location = {Thousand Oaks,~CA},
  doi = {10.4135/9781071812082},
  isbn = {978-1-0718-1212-9 978-1-0718-1208-2},
  langid = {english},
  keywords = {science,scientific methodology}
}

@article{gigerenzer2004,
  title = {Mindless statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11-01},
  journaltitle = {The Journal of Socio-Economics},
  shortjournal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {collective illusions,exact sciences,null hypothesis significance testing,null ritual,probability and statistics,rituals,statistical rituals,statistical significance,statistical thinking,textbooks}
}

@online{greener2020,
  title = {Stop testing for normality},
  author = {Greener, Robert},
  date = {2020-08-04T12:53:26},
  url = {https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90},
  urldate = {2024-09-29},
  abstract = {Normality tests are misleading and a waste of your time!},
  langid = {english},
  organization = {Medium},
  keywords = {exact sciences,general linear models,normality tests,tests}
}

@video{greinert2026,
  entrysubtype = {video},
  title = {Acampamos na {{Antártida}} ({{Ep}}. 4)},
  editor = {Greinert, Renan Augusto Guerra and Martins, Michele Oliveira},
  editortype = {director},
  date = {2026-01-27},
  url = {https://youtu.be/QtW1lQITckE?si=4_wgsHUd96Dvmy-e},
  langid = {english},
  keywords = {antactica,penguins},
  note = {We camped in Antarctica (Ep. 4)}
}

@book{grosser2021,
  title = {Advanced {{R}} solutions},
  author = {Grosser, Malte and Bumann, Henning and Wickham, Hadley},
  date = {2021},
  publisher = {CRC Press},
  location = {Boca Raton, FL},
  abstract = {This book offers solutions to all 284 exercises in Advanced R, Second Edition. All the solutions have been carefully documented and made to be as clear and accessible as possible. Working through the exercises and their solutions will give you a deeper understanding of a variety of programming challenges, many of which are relevant to everyday work. This will expand your set of tools on a technical and conceptual level. You will be able to transfer many of the specific programming schemes directly and will discover far more elegant solutions to everyday problems.Features:When R creates copies, and how it affects memory usage and code performanceEverything you could ever want to know about functionsThe differences between calling and exiting handlersHow to employ functional programming to solve modular tasksThe motivation, mechanics, usage, and limitations of R's highly pragmatic S3 OO systemThe R6 OO system, which is more like OO programming in other languagesThe rules that R uses to parse and evaluate expressionsHow to use metaprogramming to generate HTML or LaTeX with elegant R codeHow to identify and resolve performance bottlenecks},
  isbn = {978-1-032-00749-6},
  langid = {english},
  pagetotal = {302},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@online{hartnett2021,
  title = {Matrix multiplication inches closer to mythic goal},
  author = {Hartnett, Kevin},
  date = {2021-03-23},
  url = {https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323},
  urldate = {2026-01-30},
  abstract = {A recent paper set the fastest record for multiplying two matrices. But it also marks the end of the line for a method researchers have relied on for decades to make improvements.},
  langid = {english},
  organization = {Quanta Magazine},
  keywords = {computer science,exact sciences,mathematics,matrices}
}

@article{horne1976,
  title = {A self-assessment questionnaire to determine morningness-eveningness in human circadian rhythms},
  author = {Horne, J. A. and Östberg, O.},
  date = {1976},
  journaltitle = {International Journal of Chronobiology},
  shortjournal = {Int J Chronobiol},
  volume = {4},
  number = {2},
  pages = {97--110},
  issn = {0300-9998},
  abstract = {An English language self-assessment Morningness-Eveningness questionnaire is presented and evaluated against individual differences in the circadian vatiation of oral temperature. 48 subjects falling into Morning, Evening and Intermediate type categories regularly took their temperature. Circadian peak time were identified from the smoothed temperature curves of each subject. Results showed that Morning types and a significantly earlier peak time than Evening types and tended to have a higher daytime temperature and lower post peak temperature. The Intermediate type had temperatures between those of the other groups. Although no significant differences in sleep lengths were found between the three types, Morning types retired and arose significantly earlier than Evening types. Whilst these time significatly correlated with peak time, the questionnaire showed a higher peak time correlation. Although sleep habits are an important déterminant of peak time there are other contibutory factors, and these appear to be partly covered by the questionnaire. Although the questionnaire appears to be valid, further evaluation using a wider subject population is required.},
  langid = {english},
  keywords = {biological sciences,chronobiology,fundamentals of chronobiology,meq}
}

@book{howell2013,
  title = {Statistical methods for psychology},
  author = {Howell, David C.},
  date = {2013},
  edition = {8},
  publisher = {Wadsworth Cengage Learning},
  location = {Belmont, CA},
  abstract = {Statistical Methods For Psychology surveys the statistical techniques commonly used in the behavioral and social sciences, especially psychology and education. To help students gain a better understanding of the specific statistical hypothesis tests that are covered throughout the text, author David Howell emphasize conceptual understanding. Along with significantly updated discussions of effect size and meta-analysis, this Eighth Edition continues to focus on two key themes that are the cornerstones of this book's success: the importance of looking at the data before beginning a hypothesis test, and the importance of knowing the relationship between the statistical test in use and the theoretical questions being asked by the experiment.},
  isbn = {978-1-111-83548-4},
  pagetotal = {770},
  keywords = {exact sciences,fundamentals of probability and statistics,probability and statistics,psychology,psychometrics,statistical methods,t-test,textbooks}
}

@book{ifrah2001,
  title = {The universal history of computing: {{From}} the abacus to the quantum computer},
  shorttitle = {The universal history of computing},
  author = {Ifrah, Georges},
  translator = {Harding, E. F.},
  namea = {{Sophie Wood} and {Ian Monk} and {Elizabeth Clegg} and {Guido Waldman}},
  nameatype = {collaborator},
  date = {2001},
  origdate = {1994},
  publisher = {John Wiley \& Sons},
  location = {New York, NY},
  abstract = {Suppose every instrument could by command or by anticipation of need execute its function on its own; suppose that spindles could weave of their own accord, and plectra strike the strings of zithers by themselves; then craftsmen would have no need of hand-work, and masters have no need of slaves." Aristotle Called the Indiana Jones of arithmetic, Georges Ifrah embarked in 1974 on a ten-year quest to discover where numbers come from and what they say about us. His first book, the highly praised Universal History of Numbers, drew from this remarkable journey, presented the first complete account of the invention and evolution of numbers the world overand became an international bestseller. In The Universal History of Computing, Ifrah continues his exhilarating exploration into the fascinating world of numbers. In this fun, engaging but no less learned book, he traces the development of computing from the invention of the abacus to the creation of the binary system three centuries ago to the incredible conceptual, scientific, and technical achievements that made the first modern computers possible. He shows us how various cultures, scientists, and industries across the world struggled to break free of the tedious labor of mental calculation and, as a result, he reveals the evolution of the human mind. Evoking the excitement and joy that accompanied the grand mathematical undertakings throughout history, Ifrah takes us along as he revisits a multitude of cultures, from Roman times and the Chinese Common Era to twentieth-century England and America. We meet mathematicians, visionaries, philosophers, and scholars from every corner of the world and from every period of history. We witness the dead ends and regressions in the computers development, as well as the advances and illuminating discoveries. We learn about the births of the pocket calculator, the adding machine, the cash register, and even automata. We find out how the origins of the computer can be found in the European Renaissance, along with how World War II influenced the development of analytical calculation. And we explore such hot topics as numerical codes and the recent discovery of new kinds of number systems, such as "surreal" numbers. Adventurous and enthralling, The Universal History of Computing is an astonishing achievement that not only unravels the epic tale of computing, but also tells the compelling story of human intelligenceand how much farther we still have to go.},
  isbn = {0-471-39671-0},
  langid = {english},
  origlanguage = {fr},
  origlocation = {Paris},
  origpublisher = {Editions Robert Laffont},
  origtitle = {Histoire universelle des chiffres},
  pagetotal = {410},
  keywords = {computer science,exact sciences,history,human sciences}
}

@article{ihaka1996,
  title = {R: {{A}} language for data analysis and graphics},
  shorttitle = {R},
  author = {Ihaka, Ross and Gentleman, Robert},
  date = {1996-09-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {3},
  pages = {299--314},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1080/10618600.1996.10474713},
  abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
  langid = {english},
  keywords = {computer language,computer science,exact sciences,probability and statistics,r (programming language),statistical computing}
}

@video{jago2014,
  entrysubtype = {video},
  title = {Turing machines explained},
  editor = {Jago, Mark},
  editortype = {director},
  date = {2014-08-29},
  publisher = {Computerphile},
  url = {https://youtu.be/QtW1lQITckE?si=4_wgsHUd96Dvmy-e},
  abstract = {Turing Machines are the basis of modern computing, but what actually is a Turing Machine? Assistant Professor Mark Jago explains.},
  langid = {english},
  keywords = {computer science,fundamentals of computer science},
  note = {We camped in Antarctica (Ep. 4)}
}

@article{kahanwal2013,
  title = {Abstraction level taxonomy of programming language frameworks},
  author = {Kahanwal, Brijender},
  date = {2013-10},
  journaltitle = {International Journal of Programming Languages and Applications},
  shortjournal = {IJPLA},
  volume = {3},
  number = {4},
  eprint = {1311.3293},
  eprinttype = {arXiv},
  eprintclass = {cs},
  issn = {18396291},
  doi = {10.5121/ijpla.2013.3401},
  abstract = {The main purpose of this article is to describe the taxonomy of computer languages according to the levels of abstraction. There exists so many computer languages because of so many reasons like the evolution of better computer languages over the time; the socio-economic factors as the proprietary interests, commercial advantages; expressive power; ease of use of novice; orientation toward special purposes; orientation toward special hardware; and diverse ideas about most suitability. Moreover, the important common properties of most of these languages are discussed here. No programming language is designed in a vacuity, but it solves some specific kinds of problems. There is a different framework for each problem and best suitable framework for each problem. A single framework is not best for all types of problems. So, it is important to select vigilantly the frameworks supported by the language. The five generation of the computer programming languages are explored in this paper to some extent.},
  keywords = {computer science,exact sciences,programming languages,taxonomy}
}

@article{klein1992,
  title = {Integration of the microcomputer into the changing accounting curriculum: {{An}} analysis of recent findings},
  shorttitle = {Integration of the microcomputer into the changing accounting curriculum},
  author = {Klein, Lawrence A. and Cerullo, Michael J. and Cerullo, M. Virginia},
  date = {1992-10-01},
  journaltitle = {Computers \& Education},
  shortjournal = {Computers \& Education},
  volume = {19},
  number = {3},
  pages = {209--217},
  issn = {0360-1315},
  doi = {10.1016/0360-1315(92)90114-K},
  abstract = {This article reports the results of a longitudinal study concerning the utilization of microcomputers in accounting curricula. A comparative summary of three earlier studies by the authors on PC computer integration in accounting classes is also incorporated into the paper. Accounting departments are currently in a period of stabilization with respect to computer hardware and software. For the last 2 or 3 years, PC usage in accounting courses has leveled off at 62\%. Although the amount of integration may be adequate, the type of integration remains questionable. The electronic spreadsheet is the most widely employed software package in the accounting curriculum, and knowledge of spreadsheet applications has become equated with PC computer literacy. This paper emphasizes that employment of micros in selected accounting courses must progress beyond spreadsheet applications to include more sophisticated, higher level applications. The authors predict that the evolution of information systems technologies will accelerate dramatically in the 1990s. Such changes will impact the way accounting is practiced, the way management services are provided, the types of internal controls installed in systems, the way accounting systems are developed, the way managers make decisions and so on. Yet, this study reveals stagnant classroom applications which are piecemeal and nonsystems (i.e. nonplanning) in orientation with spreadsheet applications predominating. Other more significant micro applications must be integrated into the classroom. About 31\% of the accounting departments responding to the 1989 survey have the mechanism in place to bring about the needed changes in the 1990s. The recommended mechanism is a departmental PC Steering Committee. Such a committee should be formed by all accounting departments to bring about the orderly deployment of PC applications in a changing accounting curriculum. If the accounting PC Steering Committee takes its job seriously, the important information systems technologies affecting the development of accounting are more likely to be incorporated into the classroom, keeping accounting students abreast of the latest developments in the information age.}
}

@article{kozak2018,
  title = {What's normal anyway? {{Residual}} plots are more telling than significance tests when checking {{ANOVA}} assumptions},
  shorttitle = {What's normal anyway?},
  author = {Kozak, M. and Piepho, H.-P.},
  date = {2018},
  journaltitle = {Journal of Agronomy and Crop Science},
  volume = {204},
  number = {1},
  pages = {86--98},
  issn = {1439-037X},
  doi = {10.1111/jac.12220},
  abstract = {We consider two questions important for applying analysis of variance (ANOVA): Should normality be checked on the raw data or on the residuals (or is it immaterial which of the two approaches we take)? Should normality and homogeneity of variance be checked using significance tests or diagnostic plots (or both)? Based on two examples, we show that residuals should be used for model checking and that residual plots are better for checking ANOVA assumptions than statistical tests. We also discuss why one should be very cautious when using statistical tests to check the assumptions.},
  langid = {english},
  keywords = {anova,assumption checks,diagnostic plots,exact sciences,general linear models,linear models,probability and statistics,statistical assumptions}
}

@book{kuhn2022,
  title = {Tidy modeling with {{R}}: a framework for modeling in the tidyverse},
  shorttitle = {Tidy modeling with {{R}}},
  author = {Kuhn, Max and Silge, Julia},
  date = {2022},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://www.tmwr.org},
  abstract = {Get going with tidymodels, a collection of R packges for modeling and machine learning. Whether you're just starting out or have years of experience with modeling, this practical introduction shows data analysts, business analysts, and data scientists how the tidymodels framework offers a consistent, flexible approach for your work. RStudio engineers Max Kuhn and Julia Silge demonstrate ways to create models by focusing on an R dialect called the tidyverse. Software that adops tidyverse principles shares both a high-level design philosophy and low-level grammar and data structures, so learning one piece of the ecosystem makes it easier to learn the next. You'll understand why the tidymodels framework has been built to be used by a broad range of people.},
  isbn = {978-1-4920-9648-1},
  langid = {english},
  pagetotal = {363},
  keywords = {artificial intelligence,exact sciences,machine learning,modeling,probability and statistics,programming,r (programming language),tidyverse}
}

@article{leocadio-miguel2017,
  title = {Latitudinal cline of chronotype},
  author = {Leocadio-Miguel, Mario André and Louzada, Fernando Mazzili and Duarte, Leandro Lourenção and Areas, Roberta Peixoto and Alam, Marilene and Freire, Marcelo Ventura and Fontenele-Araujo, John and Menna-Barreto, Luiz and Pedrazzoli, Mario},
  date = {2017-07-14},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {7},
  number = {1},
  pages = {5437},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05797-w},
  abstract = {Abstract             The rotation of the Earth around its own axis and around the sun determines the characteristics of the light/dark cycle, the most stable and ancient 24\,h temporal cue for all organisms. Due to the tilt in the earth’s axis in relation to the plane of the earth’s orbit around the sun, sunlight reaches the Earth differentially depending on the latitude. The timing of circadian rhythms varies among individuals of a given population and biological and environmental factors underlie this variability. In the present study, we tested the hypothesis that latitude is associated to the regulation of circadian rhythm in humans. We have studied chronotype profiles across latitudinal cline from around 0° to 32° South in Brazil in a sample of 12,884 volunteers living in the same time zone. The analysis of the results revealed that humans are sensitive to the different sunlight signals tied to differences in latitude, resulting in a morning to evening latitudinal cline of chronotypes towards higher latitudes.},
  langid = {english},
  keywords = {biological sciences,chronobiology,chronotype,entrainment,latitude,meq}
}

@video{lockerbie2025,
  entrysubtype = {video},
  title = {I made my phone's chip 2,000,000× bigger and flew inside},
  editor = {Lockerbie, Toby},
  editortype = {director},
  date = {2025-12-26},
  url = {https://youtu.be/QtW1lQITckE?si=4_wgsHUd96Dvmy-e},
  langid = {english},
  keywords = {computer science,fundamentals of computer science}
}

@article{lohr2014,
  entrysubtype = {newspaper},
  title = {For big-data scientists, ‘{{Janitor}} work’ is key hurdle to insights},
  author = {Lohr, Steve},
  date = {2014-08-18},
  journaltitle = {The New York Times},
  location = {New York, NY},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html},
  urldate = {2024-06-10},
  abstract = {The analysis of giant data sets promises unique business insights, but much manual effort is still required to prepare the information for parsing.},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {data engineering,data science,exact sciences,probability and statistics}
}

@article{mariscal2021,
  title = {Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets},
  author = {Gómez-de-Mariscal, Estibaliz and Guerrero, Vanesa and Sneider, Alexandra and Jayatilaka, Hasini and Phillip, Jude M. and Wirtz, Denis and Muñoz-Barrutia, Arrate},
  date = {2021-10-22},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {20942},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-00199-5},
  abstract = {Biomedical research has come to rely on p-values as a deterministic measure for data-driven decision-making. In the largely extended null hypothesis significance testing for identifying statistically significant differences among groups of observations, a single p-value is computed from sample data. Then, it is routinely compared with a threshold, commonly set to 0.05, to assess the evidence against the hypothesis of having non-significant differences among groups, or the null hypothesis. Because the estimated p-value tends to decrease when the sample size is increased, applying this methodology to datasets with large sample sizes results in the rejection of the null hypothesis, making it not meaningful in this specific situation. We propose a new approach to detect differences based on the dependence of the p-value on the sample size. We introduce new descriptive parameters that overcome the effect of the size in the p-value interpretation in the framework of datasets with large sample sizes, reducing the uncertainty in the decision about the existence of biological differences between the compared experiments. The methodology enables the graphical and quantitative characterization of the differences between the compared experiments guiding the researchers in the decision process. An in-depth study of the methodology is carried out on simulated and experimental data. Code availability at https://github.com/BIIG-UC3M/pMoSS.},
  issue = {1},
  langid = {english},
  keywords = {exact sciences,p-value,p-value problem,power analysis,probability and statistics,sampling}
}

@article{marwick2018,
  title = {Packaging data analytical work reproducibly using {{R}} (and friends)},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1375986},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  langid = {english},
  keywords = {data science,exact sciences,programming,r (programming language),r packages}
}

@article{mcilroy1978,
  title = {{{UNIX Time-Sharing System}}: {{Forward}}},
  author = {McIlroy, M. D. and Pinson, E. N. and Tague, B. A.},
  date = {1978},
  journaltitle = {Bell System Technical Journal},
  volume = {57},
  number = {6},
  pages = {1899--1904},
  url = {https://archive.org/details/bstj57-6-1899/mode/2up},
  langid = {english},
  keywords = {best practices,computer science,exact sciences,operating systems,unix,unix philosophy}
}

@article{neyman1928,
  title = {On the use and interpretation of certain test criteria for purposes of statistical inference: {{Part I}}},
  shorttitle = {On the use and interpretation of certain test criteria for purposes of statistical inference},
  author = {Neyman, J. and Pearson, E. S.},
  date = {1928},
  journaltitle = {Biometrika},
  volume = {20A},
  number = {1/2},
  eprint = {2331945},
  eprinttype = {jstor},
  pages = {175--240},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331945},
  langid = {english},
  keywords = {exact sciences,extraordinary publications,hypothesis tests,hypothetico–deductive method,probability and statistics,statistical inference}
}

@article{neyman1928a,
  title = {On the use and interpretation of certain test criteria for purposes of statistical inference: {{Part II}}},
  shorttitle = {On the use and interpretation of certain test criteria for purposes of statistical inference},
  author = {Neyman, J. and Pearson, E. S.},
  date = {1928},
  journaltitle = {Biometrika},
  volume = {20A},
  number = {3/4},
  eprint = {2332112},
  eprinttype = {jstor},
  pages = {263--294},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2332112},
  langid = {english},
  keywords = {exact sciences,extraordinary publications,hypothesis tests,hypothetico–deductive method,probability and statistics,statistical inference}
}

@article{norde2023,
  title = {Measuring food systems sustainability in heterogenous countries: {{The Brazilian}} multidimensional index updated version applicability},
  shorttitle = {Measuring food systems sustainability in heterogenous countries},
  author = {Norde, Marina Maintinguer and Porciuncula, Laura and Garrido, Giovanna and Nunes-Galbes, Nadine Marques and Sarti, Flavia Mori and Marchioni, Dirce Maria Lobo and family=Carvalho, given=Aline Martins, prefix=de, useprefix=true},
  date = {2023},
  journaltitle = {Sustainable Development},
  volume = {31},
  number = {1},
  pages = {91--107},
  issn = {1099-1719},
  doi = {10.1002/sd.2376},
  abstract = {Countries with large territories marked by heterogeneity lack tools to monitor their distinct food systems sustainability. The multidimensional index for sustainable food systems (MISFS) was designed to measure food system sustainability locally, using the Brazilian territory. The aim was two-fold: to present the MISFS updating process (MISFS-R), and to show the applicability of MISFS-R to uncover local geopolitical priorities. Methods: Based on a systematized review and sensitivity analysis, 46 indicators were selected to compose the MISFS-R social, nutritional, environmental, and economic dimensions. Official Brazilian datasets allowed the MISFS-R scoring for 26 Brazilian states and capitals. Correlations between MISFS-R dimensions were analyzed by Spearman's coefficient. A cluster analysis was used to check for Brazilian states similarities and priorities. Findings: Economic and environmental performance were inversely related, while economic performance was directly related to social and nutritional scores. Four clusters were formed with marked differences in their food systems sustainability profile.},
  langid = {english},
  keywords = {brazil,economic,environmental,food systems,health sciences,misfs,nutrition,nutritional,social,sustainability}
}

@book{papert2020,
  title = {Mindstorms: {{Children}}, computers, and powerful ideas},
  shorttitle = {Mindstorms},
  author = {Papert, Seymour},
  date = {2020},
  publisher = {Basic Books},
  location = {New York, NY},
  abstract = {In this book, pioneering computer scientist Seymour Papert uses the invention of LOGO, the first child-friendly programming language, to make the case for the value of teaching children with computers. Papert argues that children are more than capable of mastering computers, and that teaching computational processes like de-bugging in the classroom can change the way we learn everything else. He also shows that schools saturated with technology can actually improve socialization and interaction among students and between students and teachers},
  isbn = {978-1-5416-7512-4},
  langid = {english},
  pagetotal = {265},
  keywords = {computer science,computer simulations,constructionism,education,educational computing,exact sciences,human sciences,logo}
}

@book{peng2022,
  title = {R programming for data science},
  author = {Peng, Roger D.},
  date = {2022-05-31},
  url = {https://bookdown.org/rdpeng/rprogdatascience},
  urldate = {2025-02-25},
  abstract = {The R programming language has become the de facto programming language for data science. Its flexibility, power, sophistication, and expressiveness have made it an invaluable tool for data scientists around the world. This book is about the fundamentals of R programming. You will get started with the basics of the language, learn how to manipulate datasets, how to write functions, and how to debug and optimize code. With the fundamentals provided in this book, you will have a solid foundation on which to build your data science toolbox.},
  langid = {english},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,programming languages,r (programming language),textbooks}
}

@article{perezgonzalez2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} tutorial for teaching data testing},
  shorttitle = {Fisher, {{Neyman-Pearson}} or {{NHST}}?},
  author = {Perezgonzalez, Jose D.},
  date = {2015-03-02},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00223},
  abstract = {Despite frequent calls for the overhaul of null hypothesis significance testing (NHST), this controversial procedure remains ubiquitous in behavioral, social and biomedical teaching and research. Little change seems possible once the procedure becomes well ingrained in the minds and current practice of researchers; thus, the optimal opportunity for such change is at the time the procedure is taught, be this at undergraduate or at postgraduate levels. This paper presents a tutorial for the teaching of data testing procedures, often referred to as hypothesis testing theories. The first procedure introduced is Fisher's approach to data testing—tests of significance; the second is Neyman-Pearson's approach—tests of acceptance; the final procedure is the incongruent combination of the previous two theories into the current approach—NSHT. For those researchers sticking with the latter, two compromise solutions on how to improve NHST conclude the tutorial.},
  langid = {english},
  keywords = {exact sciences,fisher,history,hypothesis tests,hypothetico–deductive method,neyman-pearson,null hypothesis significance testing,science,statistical education,teaching statistics,test of significance,test of statistical hypothesis}
}

@book{popper1979a,
  title = {Objective knowledge: {{An}} evolutionary approach},
  shorttitle = {Objective knowledge},
  author = {Popper, Karl Raimund},
  date = {1979},
  origdate = {1972},
  publisher = {Oxford University Press},
  location = {Oxford, UK},
  isbn = {0-19-875024-2},
  langid = {english},
  pagetotal = {395},
  keywords = {epistemology,ontology,philosophy,science,scientific methodology}
}

@software{rcoreteam,
  title = {R: {{A}} language and environment for statistical computing},
  author = {{R Core Team}},
  location = {Vienna, Austria},
  url = {https://www.r-project.org},
  organization = {R Foundation for Statistical Computing},
  keywords = {computer science,exact sciences,nosource,probability and statistics,programming languages,r (programming language)}
}

@book{reis2022,
  title = {Fundamentals of data engineering: plan and build robust data systems},
  shorttitle = {Fundamentals of data engineering},
  author = {Reis, Joe and Housley, Matt},
  date = {2022},
  publisher = {O'Reilly},
  location = {Sebastopol, CA},
  isbn = {978-1-0981-0830-4},
  langid = {english},
  keywords = {data engineering,engineering,exact sciences,fundamentals of data engineering,information science}
}

@article{rowley2007,
  title = {The wisdom hierarchy: representations of the {{DIKW}} hierarchy},
  shorttitle = {The wisdom hierarchy},
  author = {Rowley, Jennifer},
  date = {2007-04-01},
  journaltitle = {Journal of Information Science},
  volume = {33},
  number = {2},
  pages = {163--180},
  publisher = {SAGE Publications Ltd},
  issn = {0165-5515},
  doi = {10.1177/0165551506070706},
  abstract = {This paper revisits the data-information-knowledge-wisdom (DIKW) hierarchy by examining the articulation of the hierarchy in a number of widely read textbooks, and analysing their statements about the nature of data, information, knowledge, and wisdom. The hierarchy referred to variously as the ‘Knowledge Hierarchy’, the ‘Information Hierarchy’ and the ‘Knowledge Pyramid’ is one of the fundamental, widely recognized and ‘taken-for-granted’ models in the information and knowledge literatures. It is often quoted, or used implicitly, in definitions of data, information and knowledge in the information management, information systems and knowledge management literatures, but there has been limited direct discussion of the hierarchy. After revisiting Ackoff’s original articulation of the hierarchy, definitions of data, information, knowledge and wisdom as articulated in recent textbooks in information systems and knowledge management are reviewed and assessed, in pursuit of a consensus on definitions and transformation processes. This process brings to the surface the extent of agreement and dissent in relation to these definitions, and provides a basis for a discussion as to whether these articulations present an adequate distinction between data, information, and knowledge. Typically information is defined in terms of data, knowledge in terms of information, and wisdom in terms of knowledge, but there is less consensus in the description of the processes that transform elements lower in the hierarchy into those above them, leading to a lack of definitional clarity. In addition, there is limited reference to wisdom in these texts.},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science}
}

@article{schucany2006,
  title = {Preliminary goodness-of-fit tests for normality do not validate the one-sample {{Student}} t},
  author = {Schucany, William R. and Ng, H. K. Tony},
  date = {2006-12-01},
  journaltitle = {Communications in Statistics - Theory and Methods},
  volume = {35},
  number = {12},
  pages = {2275--2286},
  publisher = {Taylor \& Francis Group},
  doi = {10.1080/03610920600853308},
  abstract = {One of the most basic topics in many introductory statistical methods texts is inference for a population mean, μ. The primary tool for confidence intervals and tests is the Student t sampling dist...},
  langid = {english},
  keywords = {assumption checks,exact sciences,general linear models,normality tests,probability and statistics,statistical assumptions,tests}
}

@article{shatz2024,
  title = {Assumption-checking rather than (just) testing: {{The}} importance of visualization and effect size in statistical diagnostics},
  shorttitle = {Assumption-checking rather than (just) testing},
  author = {Shatz, Itamar},
  date = {2024-02-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {56},
  number = {2},
  pages = {826--845},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02072-x},
  abstract = {Statistical methods generally have assumptions (e.g., normality in linear regression models). Violations of these assumptions can cause various issues, like statistical errors and biased estimates, whose impact can range from inconsequential to critical. Accordingly, it is important to check these assumptions, but this is often done in a flawed way. Here, I first present a prevalent but problematic approach to diagnostics—testing assumptions using null hypothesis significance tests (e.g., the Shapiro–Wilk test of normality). Then, I consolidate and illustrate the issues with this approach, primarily using simulations. These issues include statistical errors (i.e., false positives, especially with large samples, and false negatives, especially with small samples), false binarity, limited descriptiveness, misinterpretation (e.g., of p-value as an~effect size), and potential testing failure due to unmet test assumptions. Finally, I synthesize the implications of these issues for statistical diagnostics, and provide practical recommendations for improving such diagnostics. Key recommendations include maintaining awareness of the issues with assumption tests (while recognizing they can be useful), using appropriate combinations of diagnostic methods (including visualization and effect sizes) while recognizing their limitations, and distinguishing between testing and checking assumptions. Additional recommendations include judging assumption violations as a complex spectrum (rather than a simplistic binary), using programmatic tools that increase replicability and decrease researcher degrees of freedom, and sharing the material and rationale involved in the diagnostics.},
  langid = {english},
  keywords = {assumption checks,exact sciences,general linear models,graphical methods,null hypothesis significance testing,probability and statistics,statistical assumptions,statistical diagnostics,visualization}
}

@dataset{sisvan,
  title = {Microdados dos acompanhamentos de estado nutricional},
  author = {{Sistema de Vigilância Alimentar e Nutricional} and {Coordenação-Geral de Alimentação e Nutrição} and {Departamento de Promoção da Saúde} and {Coordenação Setorial de Tecnologia da Informação} and {Secretaria de Atenção Primária à Saúde} and {Ministério da Saúde}},
  publisher = {openDataSUS},
  url = {https://dadosabertos.saude.gov.br/dataset/sisvan-estado-nutricional},
  abstract = {O Ministério da Saúde, por meio da Coordenação-Geral de Alimentação e Nutrição (CGAN) do Departamento de Promoção da Saúde (DEPROS) e da Coordenação Setorial de Tecnologia da Informação (COSTI) da Secretaria de Atenção Primária à Saúde (SAPS), realizam a gestão da base nacional do Sistema de Vigilância Alimentar e Nutricional (Sisvan). O Sisvan tem por objetivo consolidar os dados referentes às ações de Vigilância Alimentar e Nutricional (VAN) da Atenção Primária à Saúde (APS). A versão totalmente eletrônica do Sisvan apresenta dados de antropometria e de marcadores do consumo alimentar desde 2008. Deste modo, agrega os registros de estado nutricional provenientes do Sistema de Gestão do Programa Auxílio Brasil (atual programa de transferência condicionada de renda denominado até meados de 2021, como Programa Bolsa Família), bem como os registros de estado nutricional e de marcadores do consumo alimentar inseridos pelo e-SUS APS. E considerando que os dados do Sisvan são individualizados e organizados segundo CNS (Cartão Nacional de Saúde), os relatórios obtidos a partir do Sisvan permitem a avaliação de frequências relativas, logo, fornecem prevalências de estado nutricional e de marcadores do consumo alimentar da população atendida na APS. Conteúdo As bases de dados disponibilizadas nesta Plataforma referem-se aos dados individualizados e anonimizados de todas as pessoas atendidas quanto ao estado nutricional (antropometria) em serviços de APS do Brasil desde 2008. Neste sentido, poderão ser identificados um ou mais registros de antropometria para uma mesma pessoa. Esta medida permite a cessão de dados contidos nas bases nacionais dos sistemas de informação em saúde (que é o caso do Sisvan) e, portanto, cabe o atendimento à Lei de Acesso à Informação e Lei Geral de Proteção de Dados, por isso os dados serão disponibilizados com sigilo da informação pessoal, não sendo permitido o rastreio ou identificação de qualquer cidadão, mas poderá ser avaliada a prevalência de indicadores nutricionais. Limitações Conforme indicado no conteúdo, as bases são disponibilizadas com várias origens de dados, dessa forma, pode haver mais de um acompanhamento por indivíduo. Orientamos sempre em priorizar acompanhamentos gerados pelo SISVAN, AUXÍLIO BRASIL/BOLSA FAMÍLIA e por último E-SUS, nos casos de acompanhamentos com a mesma data. Os relatórios públicos do SISVAN consolidam sempre o último acompanhamento do indivíduo, dessa forma, realizar uma comparação entre a base disponibilizada e o consolidado do sistema, pode ocasionar uma diferença razoável, visto que existem dados sendo incorporados semanalmente.},
  langid = {brazilian},
  keywords = {brazil,citizen science,exact sciences,nosource,nutrition,nutritional status,open data,open science,probability and statistics,sisvan},
  note = {Microdata on nutritional status monitoring}
}

@article{student1908,
  title = {The probable error of a mean},
  author = {{Student}},
  date = {1908},
  journaltitle = {Biometrika},
  volume = {6},
  number = {1},
  eprint = {2331554},
  eprinttype = {jstor},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331554},
  langid = {english},
  keywords = {classical publications,exact sciences,probability and statistics,t-test}
}

@video{thenorio2022,
  entrysubtype = {video},
  title = {A saga dos computadores},
  editor = {Thenório, Iberê},
  editortype = {director},
  date = {2022-03-22},
  url = {https://www.youtube.com/playlist?list=PLYjrJH3e_wDOA5mxhiMxE6yslcIzU5NkX},
  urldate = {2024-08-04},
  abstract = {💻📟🧠 Computador: essa palavra faz parte da nossa rotina. A gente usa todos os dias, seja pra estudar, trabalhar, se divertir ou se conectar com o mundo. Mas… você sabe como essa máquina incrível funciona de verdade? E de onde ela surgiu? Na playlist “A Saga dos Computadores”, o Manual do Mundo abre a caixa-preta da computação e te leva em uma viagem épica pelos bastidores dessa invenção que mudou tudo. Com vídeos curtos, objetivos e recheados de explicações visuais, Iberê Thenório apresenta os conceitos por trás do funcionamento dos computadores de um jeito claro, acessível e envolvente. É o tipo de conteúdo que todo mundo deveria assistir pelo menos uma vez na vida — afinal, o computador é uma das maiores criações da humanidade. Nesta série, você vai entender desde os primeiros “computadores humanos” até os chips de inteligência artificial, passando por válvulas, transistores, placas-mãe e muita história. Tudo com o rigor técnico e a linguagem simples que já são marca registrada do canal.},
  langid = {brazilian},
  keywords = {computer science,fundamentals of computer science},
  note = {The computer saga}
}

@article{tisue2004,
  title = {{{NetLogo}}: {{A}} simple environment for modeling complexity},
  shorttitle = {{{NetLogo}}},
  author = {Tisue, Seth and Wilensky, Uri},
  date = {2004-05-16},
  journaltitle = {Center for Connected Learning and Computer-Based Modeling},
  url = {http://www.ccl.sesp.northwestern.edu/papers/netlogo-iccs2004.pdf},
  abstract = {NetLogo [Wilensky, 1999] is a multi-agent programming language and modeling environment for simulating complex phenomena. It is designed for both research and education and is used across a wide range of disciplines and education levels. In this paper we focus on NetLogo as a tool for research and for teaching at the undergraduate level and higher. We outline the principles behind our design and describe recent and planned enhancements.},
  langid = {english},
  keywords = {agent-based modeling,complexity science,computer science,exact sciences,interdisciplinary fields,modeling,netlogo,programming languages}
}

@software{ushey2025,
  title = {{{renv}}: {{Project}} environments},
  shorttitle = {{{renv}}},
  author = {Ushey, Kevin and Wickham, Hadley},
  date = {2025},
  doi = {10.32614/CRAN.package.renv},
  abstract = {A dependency management toolkit for R. Using 'renv', you can create and manage project-local R libraries, save the state of these libraries to a 'lockfile', and later restore your library as required. Together, these tools can help make your projects more isolated, portable, and reproducible.},
  keywords = {computer science,exact sciences,programming,r (programming language),r packages}
}

@book{vanderloo2018,
  title = {Statistical data cleaning with applications in {{R}}},
  author = {family=Loo, given=Mark, prefix=van der, useprefix=true and family=Jonge, given=Edwin, prefix=de, useprefix=false},
  date = {2018},
  publisher = {John Wiley \& Sons},
  location = {Hoboken, NJ},
  abstract = {A comprehensive guide to automated statistical data cleaning  The production of clean data is a complex and time-consuming process that requires both technical know-how and statistical expertise. Statistical Data Cleaning brings together a wide range of techniques for cleaning textual, numeric or categorical data. This book examines technical data cleaning methods relating to data representation and data structure. A prominent role is given to statistical data validation, data cleaning based on predefined restrictions, and data cleaning strategy. Key features: - Focuses on the automation of data cleaning methods, including both theory and applications written in R.   - Enables the reader to design data cleaning processes for either one-off analytical purposes or for setting up production systems that clean data on a regular basis.   - Explores statistical techniques for solving issues such as incompleteness, contradictions and outliers, integration of data cleaning components and quality monitoring.   - Supported by an accompanying website featuring data and R code. This book enables data scientists and statistical analysts working with data to deepen their understanding of data cleaning as well as to upgrade their practical data cleaning skills. It can also be used as material for a course in data cleaning and analyses.},
  isbn = {978-1-118-89714-0},
  langid = {english},
  pagetotal = {300},
  keywords = {computer science,data munging,data science,exact sciences,probability and statistics,r (programming language)}
}

@thesis{vartanian2024a,
  type = {Corrected version},
  title = {Is latitude associated with chronotype?},
  author = {Vartanian, Daniel},
  date = {2024-11-27},
  institution = {University of São Paulo},
  location = {São Paulo, SP},
  doi = {10.11606/D.100.2025.tde-02042025-063648},
  abstract = {Although significant progress has been made in understanding circadian rhythms, further research with larger and more diverse samples is needed to deepen our understanding of temporal phenotypes and their variability. This thesis examines the relationship between latitude and human chronotype expression, investigating whether variations in annual sunlight exposure between equatorial and non-equatorial regions influence circadian phenotypes. The underlying premise suggests that a stronger solar zeitgeber near the equator should promote greater entrainment to the light/dark cycle, potentially reducing phenotype diversity and favoring morningness in equatorial populations. To test this hypothesis, data from 65,824 individuals distributed across a 33.85° latitude range in Brazil were analyzed. Data collection employed the Munich ChronoType Questionnaire (MCTQ) during a single spring week (October 15-21, 2017), minimizing seasonal variations in photoperiod across regions. The analysis employed nested regression models weighted according to population proportions at the time of data collection. Contrary to expectations, results revealed no meaningful relationship between latitude and chronotype (Cohen's f2 = 0.00308, 95\% CI[0, 0.01214]), consistent with recent findings in the field. All analytical procedures, from raw data processing through effect size estimation, were conducted using reproducible methods. These findings contribute to our evidence-based understanding of circadian rhythm regulation while challenging established assumptions in chronobiology research. While this study does not refute the hypothesis outright, the association between latitude and chronotype should remain an open scientific question rather than settled knowledge until robust evidence confirms it.},
  langid = {english},
  pagetotal = {54},
  keywords = {biological sciences,chronobiology,chronotype,complexity science,entrainment,latitude,mctq,mctq (r package)}
}

@article{vonneumann1993a,
  title = {First draft of a report on the {{EDVAC}}},
  author = {family=Neumann, given=John, prefix=von, useprefix=true},
  date = {1993},
  journaltitle = {IEEE Annals of the History of Computing},
  volume = {15},
  number = {4},
  pages = {27--75},
  issn = {1934-1547},
  doi = {10.1109/85.238389},
  abstract = {The first draft of a report on the EDVAC written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine (ACE) as the definitive source for understanding the nature and design of a general-purpose digital computer.},
  langid = {english},
  keywords = {computer science,electrical engineering,engines,exact sciences,extraordinary publications,forward contracts,history,laboratories,mathematics,pain,physics computing,proposals,statistics,stored-program concept}
}

@book{vygotsky2012,
  title = {Thought and language},
  author = {Vygotsky, Lev Semyonovich},
  translator = {Hanfmann, Eugenia and Vakar, Gertruda and Kozulin, Alex},
  date = {2012},
  origdate = {1934},
  edition = {Rev. exp. ed.},
  publisher = {The MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-51771-3},
  langid = {english},
  pagetotal = {307},
  keywords = {cultural-historical theory,human sciences,learning,psychology,theory of mind,vygotsky}
}

@article{wasserstein2016,
  title = {The {{ASA}} statement on p-values: {{Context}}, process, and purpose},
  shorttitle = {The {{ASA}} statement on p-values},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  volume = {70},
  number = {2},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  langid = {english},
  keywords = {consensus,exact sciences,p-value,p-value problem,probability and statistics}
}

@book{wickham2016a,
  title = {ggplot2: {{Elegant}} graphics for data analysis},
  shorttitle = {ggplot2},
  author = {Wickham, Hadley},
  namea = {Sievert, Carson},
  nameatype = {collaborator},
  date = {2016},
  series = {Use {{R}}!},
  edition = {2},
  publisher = {Springer},
  location = {Cham, Switzerland},
  doi = {10.1007/978-3-319-24277-4},
  abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to:produce handsome, publication-quality plots with automatic legends created from the plot specificationsuperimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scalesadd customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plotsapproach a graph from a visual perspective, thinking about how each component of the data is represented on the final plotThis book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R).~ ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
  isbn = {978-3-319-24277-4},
  langid = {english},
  pagetotal = {260},
  keywords = {computer science,data science,data visualization,ggplot2,grammar of graphics,probability and statistics,r (programming language)}
}

@book{wickham2019,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  date = {2019},
  edition = {2},
  publisher = {CRC Press},
  location = {Boca Raton},
  url = {https://adv-r.hadley.nz},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimising your code. By reading this book, you will learn: - The difference between an object and its name, and why the distinction is important - The important vector data structures, how they fit together, and how you can pull them apart using subsetting - The fine details of functions and environments - The condition system, which powers messages, warnings, and errors - The powerful functional programming paradigm, which can replace many for loops - The three most important OO systems: S3, S4, and R6 - The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation - Effective debugging techniques that you can deploy, regardless of how your code is run - How to find and remove performance bottlenecks The second edition is a comprehensive update: - New foundational chapters: "Names and values," "Control flow," and "Conditions" - Comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them - Much deeper coverage of metaprogramming, including the new tidy evaluation framework - Use of new package like rlang (rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (purrr.tidyverse.org/) for functional programming - Use of color in code chunks and figures},
  isbn = {978-0-8153-8457-1},
  langid = {english},
  pagetotal = {588},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@online{wickham2023c,
  title = {The tidy tools manifesto},
  author = {Wickham, Hadley},
  date = {2023},
  url = {https://tidyverse.tidyverse.org/articles/manifesto.html},
  urldate = {2023-07-18},
  abstract = {tidyverse},
  langid = {english},
  organization = {Tidyverse},
  keywords = {data engineering,data science,engineering,guia de estilo,nosource,programming,r (programming language),software engineering}
}

@book{wickham2023d,
  title = {R packages},
  author = {Wickham, Hadley and Bryan, Jennifer},
  date = {2023},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r-pkgs.org},
  urldate = {2024-07-07},
  abstract = {Turn your R code into packages that others can easily install and use. With this fully updated edition, developers and data scientists will learn how to bundle reusable R functions, sample data, and documentation together by applying the package development philosophy used by the team that maintains the "tidyverse" suite of packages. In the process, you'll learn how to automate common development tasks using a set of R packages, including devtools, usethis, testthat, and roxygen2. Authors Hadley Wickham and Jennifer Bryan from Posit (formerly known as RStudio) help you create packages quickly, then teach you how to get better over time. You'll be able to focus on what you want your package to do as you progressively develop greater mastery of the structure of a package.},
  isbn = {978-1-0981-3494-5},
  langid = {english},
  pagetotal = {381},
  keywords = {data science,engineering,exact sciences,programming,r (programming language),r packages,software engineering}
}

@book{wickham2023e,
  title = {R for data science: {{Import}}, tidy, transform, visualize, and model data},
  shorttitle = {R for data science},
  author = {Wickham, Hadley and Çetinkaya-Rundel, Mine and Grolemund, Garrett},
  date = {2023},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r4ds.hadley.nz},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverseâ??a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way.},
  isbn = {978-1-4920-9740-2},
  langid = {american},
  pagetotal = {576},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming,r (programming language)}
}

@book{wickhama,
  title = {The tidyverse style guide},
  author = {Wickham, Hadley},
  url = {https://style.tidyverse.org},
  langid = {english},
  keywords = {computer science,exact sciences,guia de estilo,nosource,r (programming language),standards}
}

@book{wickhamc,
  title = {Tidy design principles},
  author = {Wickham, Hadley},
  url = {https://design.tidyverse.org},
  langid = {english},
  keywords = {computer science,exact sciences,guia de estilo,nosource,r (programming language),standards}
}

@book{wilkinson2005,
  title = {The grammar of graphics},
  author = {Wilkinson, Leland},
  editor = {Chambers, J. and Hand, D. and Härdle, W.},
  editortype = {redactor},
  namea = {Wills, Graham and Rope, Dan and Norton, Andrew and Dubbs, Roger},
  nameatype = {collaborator},
  date = {2005},
  series = {Statistics and {{Computing}}},
  edition = {2},
  publisher = {Springer},
  location = {New York, NY},
  abstract = {Before writing the graphics for SYSTAT in the 1980’s, I began by teaching a seminar in statistical graphics and collecting as many different quantitative graphics as I could find. I was determined to produce a package that could draw every statistical graphic I had ever seen. The structure of the program was a collection of procedures named after the basic graph types they p- duced. The graphics code was roughly one and a half megabytes in size. In the early 1990’s, I redesigned the SYSTAT graphics package using - ject-based technology. I intended to produce a more comprehensive and - namic package. I accomplished this by embedding graphical elements in a tree structure. Rendering graphics was done by walking the tree and editing worked by adding and deleting nodes. The code size fell to under a megabyte. In the late 1990’s, I collaborated with Dan Rope at the Bureau of Labor Statistics and Dan Carr at George Mason University to produce a graphics p- duction library called GPL, this time in Java. Our goal was to develop graphics components. This book was nourished by that project. So far, the GPL code size is under half a megabyte.},
  isbn = {978-0-387-24544-7},
  langid = {english},
  pagetotal = {690},
  keywords = {computer science,data science,data visualization,design,exact sciences,probability and statistics}
}

@video{zaidan2025,
  entrysubtype = {video},
  title = {How are microchips made?},
  editor = {Zaidan, George and Saini, Sajan},
  editortype = {director},
  date = {2025-02-25},
  publisher = {TED-Ed},
  url = {https://youtu.be/IkRXpFIRUl4?si=iQ7xQuFS6DZLuBY7},
  urldate = {2024-08-04},
  abstract = {Globally, we produce more than a trillion computer chips every year. Which means about 20 trillion transistors are built every second— and this process is done in fewer than 500 fabrication plants. How do we build so many tiny, intricately-connected devices, so incredibly fast? George Zaidan and Sajan Saini explore how photolithography helps build these devices and its environmental impact. Lesson by George Zaidan and Sajan Saini, directed by Kozmonot Animation Studio.},
  langid = {english},
  keywords = {computer science,fundamentals of computer science}
}
