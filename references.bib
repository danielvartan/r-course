@article{ackoff1989,
  title = {From data to wisdom},
  author = {Ackoff, Russell},
  date = {1989},
  journaltitle = {Journal of Applied Systems Analysis},
  volume = {16},
  pages = {3--9},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ackoff - 1989 - From data to wisdom.pdf}
}

@online{broman2013,
  title = {Data science is statistics},
  author = {Broman, Karl},
  date = {2013-04-05T13:12:35+00:00},
  url = {https://kbroman.wordpress.com/2013/04/05/data-science-is-statistics/},
  urldate = {2024-06-10},
  langid = {english},
  organization = {The stupidest thing...},
  keywords = {data science,exact sciences,probability and statistics}
}

@book{brookshear2020,
  title = {Computer science: {{An}} overview},
  shorttitle = {Computer science},
  author = {Brookshear, J. Glenn and Brylow, Dennis},
  date = {2020},
  edition = {13 ed., Global edition},
  publisher = {Pearson},
  location = {Harlow, UK},
  abstract = {Computer Science: An Overview, 13th edition, Global Edition, by J. Glenn Brookshear, and Dennis Brylow, is written for students from all backgrounds, giving you a bottom-up, concrete-to-abstract foundation in the subject. Its broad coverage encourages a practical and realistic understanding of computer science, covering all the major concepts. The book's broad background exposes beginning computer science students to the breadth of the subject they plan to major in and teaches students from other backgrounds how to relate to the technical society in which they live. Learn in a flexible way with independent chapters you can study in any order with full-colour design to help you engage with the information. The text also uses Python to provide programming tools for exploration and experimentation in your learning. This 13th edition has been corrected and updated in each chapter to refine your learning experience. With more than 1,000 questions and exercises, the book trains your thinking skills with useful chapter review problems and contains questions surrounding social issues to reinforce core concepts. This text is comprehensive and highly accessible, making it ideal for undergraduate studies in computer science.},
  isbn = {978-1-292-26344-1},
  langid = {english},
  pagetotal = {735},
  keywords = {computer science,exact sciences,fundamentals of computer science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Brookshear - 2020 - Computer science.pdf}
}

@article{cao2017,
  title = {Data science: a comprehensive overview},
  shorttitle = {Data science},
  author = {Cao, Longbing},
  date = {2017-06-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {3},
  pages = {43},
  issn = {0360-0300},
  doi = {10.1145/3076253},
  url = {https://dl.acm.org/doi/10.1145/3076253},
  urldate = {2024-06-10},
  abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/U4EIKQ7Q/Cao - 2017 - Data Science A Comprehensive Overview.pdf}
}

@book{chang2018,
  title = {R graphics cookbook: {{Practical}} recipes for visualizing data},
  shorttitle = {R graphics cookbook},
  author = {Chang, Winston},
  date = {2018-12-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Beijing Boston Farnham},
  url = {https://r-graphics.org},
  abstract = {This O’Reilly cookbook provides more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly―without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.  Most of the recipes in this second edition use the updated version of the ggplot2 package, a powerful and flexible way to make graphs in R. You’ll also find expanded content about the visual design of graphics. If you have at least a basic understanding of the R language, you’re ready to get started with this easy-to-use reference. Use R’s default graphics for quick exploration of data Create a variety of bar graphs, line graphs, and scatter plots Summarize data distributions with histograms, density curves, box plots, and more Provide annotations to help viewers interpret data Control the overall appearance of graphics Explore options for using colors in plots Create network graphs, heat maps, and 3D scatter plots Get your data into shape using packages from the tidyverse},
  isbn = {978-1-4919-7860-3},
  langid = {english},
  pagetotal = {441},
  keywords = {computer science,data science,data visualization,design,exact sciences,ggplot2,grammar of graphics,r (programming language),textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Chang - 2018 - R graphics cookbook Practical recipes for visualizing data.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Chang - 2018 - R graphics cookbook Practical recipes for visualizing data.pdf}
}

@book{cohen1988a,
  title = {Statistical power analysis for the behavioral sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2},
  publisher = {Lawrence Erlbaum Associates},
  location = {Hillsdale, N.J},
  isbn = {978-0-8058-0283-2},
  pagetotal = {567},
  keywords = {effect size,exact sciences,power analysis,probability and statistics,sampling,textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Cohen - 1988 - Statistical power analysis for the behavioral sciences.pdf}
}

@article{cohen1992,
  title = {A power primer},
  author = {Cohen, Jacob},
  date = {1992},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {112},
  number = {1},
  pages = {155--159},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.112.1.155},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.112.1.155},
  urldate = {2023-07-18},
  langid = {english},
  keywords = {effect size,exact sciences,power analysis,probability and statistics,sampling},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Cohen - 1992 - A power primer.pdf}
}

@book{coronel2019,
  title = {Database systems: design, implementation, and management},
  shorttitle = {Database systems},
  author = {Coronel, Carlos and Morris, Steven A.},
  date = {2019},
  edition = {13},
  publisher = {Cengage},
  location = {Boston, MA},
  isbn = {978-1-337-62790-0},
  langid = {english},
  pagetotal = {802},
  keywords = {computer science,databases,exact sciences,information science,modeling,relational databases},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Coronel - 2019 - Database systems.pdf}
}

@article{dhar2023,
  title = {Data science and prediction},
  author = {Dhar, Vasant},
  date = {2023-12-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {64--73},
  issn = {0001-0782},
  doi = {10.1145/2500499},
  url = {https://dl.acm.org/doi/10.1145/2500499},
  urldate = {2024-06-10},
  abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Dhar - 2023 - Data science and prediction.pdf}
}

@article{ellis2018,
  title = {How to share data for collaboration},
  author = {Ellis, Shannon E. and Leek, Jeffrey T.},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  eprint = {32981941},
  eprinttype = {pmid},
  pages = {53--57},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375987},
  url = {https://doi.org/10.1080/00031305.2017.1375987},
  urldate = {2024-06-17},
  abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
  keywords = {data science,exact sciences,interdisciplinary fields,probability and statistics,tidy data},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ellis - 2018 - How to share data for collaboration.pdf}
}

@online{gavin2020,
  title = {Big data will be dead in 5 years},
  author = {Gavin, Lewis},
  date = {2020-10-20},
  url = {https://towardsdatascience.com/big-data-will-be-dead-in-5-years-ef4344269aef},
  urldate = {2024-06-10},
  abstract = {Everything will be big data, so it won't need a special name.},
  langid = {english},
  organization = {Towards Data Science},
  keywords = {big data,data science,exact sciences,probability and statistics}
}

@online{gofairinitiative,
  title = {{{GO FAIR}} initiative: make your data \& services {{FAIR}}},
  shorttitle = {{{GO FAIR}} initiative},
  author = {{GO FAIR initiative}},
  url = {https://www.go-fair.org/},
  urldate = {2024-06-10},
  abstract = {A bottom-up international approach for the practical implementation of the European Open Science Cloud (EOSC) as part of a global Internet of FAIR Data \& Services},
  langid = {american},
  organization = {GO FAIR},
  keywords = {data engineering,open data,open science,science}
}

@book{grosser2021,
  title = {Advanced {{R}} solutions},
  author = {Grosser, Malte and Bumann, Henning and Wickham, Hadley},
  date = {2021-08-24},
  publisher = {CRC Press},
  location = {Boca Raton, FL},
  abstract = {This book offers solutions to all 284 exercises in Advanced R, Second Edition. All the solutions have been carefully documented and made to be as clear and accessible as possible. Working through the exercises and their solutions will give you a deeper understanding of a variety of programming challenges, many of which are relevant to everyday work. This will expand your set of tools on a technical and conceptual level. You will be able to transfer many of the specific programming schemes directly and will discover far more elegant solutions to everyday problems.Features:When R creates copies, and how it affects memory usage and code performanceEverything you could ever want to know about functionsThe differences between calling and exiting handlersHow to employ functional programming to solve modular tasksThe motivation, mechanics, usage, and limitations of R's highly pragmatic S3 OO systemThe R6 OO system, which is more like OO programming in other languagesThe rules that R uses to parse and evaluate expressionsHow to use metaprogramming to generate HTML or LaTeX with elegant R codeHow to identify and resolve performance bottlenecks},
  isbn = {978-1-032-00749-6},
  langid = {english},
  pagetotal = {302},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@book{ifrah2001,
  title = {The universal history of computing: {{From}} the abacus to the quantum computer},
  shorttitle = {The universal history of computing},
  author = {Ifrah, Georges},
  translator = {Harding, E. F.},
  namea = {{Sophie Wood} and {Ian Monk} and {Elizabeth Clegg} and {Guido Waldman}},
  nameatype = {collaborator},
  date = {2001},
  origdate = {1994},
  publisher = {John Wiley \& Sons},
  location = {New York, NY},
  abstract = {Suppose every instrument could by command or by anticipation of need execute its function on its own; suppose that spindles could weave of their own accord, and plectra strike the strings of zithers by themselves; then craftsmen would have no need of hand-work, and masters have no need of slaves." Aristotle Called the Indiana Jones of arithmetic, Georges Ifrah embarked in 1974 on a ten-year quest to discover where numbers come from and what they say about us. His first book, the highly praised Universal History of Numbers, drew from this remarkable journey, presented the first complete account of the invention and evolution of numbers the world overand became an international bestseller. In The Universal History of Computing, Ifrah continues his exhilarating exploration into the fascinating world of numbers. In this fun, engaging but no less learned book, he traces the development of computing from the invention of the abacus to the creation of the binary system three centuries ago to the incredible conceptual, scientific, and technical achievements that made the first modern computers possible. He shows us how various cultures, scientists, and industries across the world struggled to break free of the tedious labor of mental calculation and, as a result, he reveals the evolution of the human mind. Evoking the excitement and joy that accompanied the grand mathematical undertakings throughout history, Ifrah takes us along as he revisits a multitude of cultures, from Roman times and the Chinese Common Era to twentieth-century England and America. We meet mathematicians, visionaries, philosophers, and scholars from every corner of the world and from every period of history. We witness the dead ends and regressions in the computers development, as well as the advances and illuminating discoveries. We learn about the births of the pocket calculator, the adding machine, the cash register, and even automata. We find out how the origins of the computer can be found in the European Renaissance, along with how World War II influenced the development of analytical calculation. And we explore such hot topics as numerical codes and the recent discovery of new kinds of number systems, such as "surreal" numbers. Adventurous and enthralling, The Universal History of Computing is an astonishing achievement that not only unravels the epic tale of computing, but also tells the compelling story of human intelligenceand how much farther we still have to go.},
  isbn = {0-471-39671-0},
  langid = {english},
  origlanguage = {fr},
  origlocation = {Paris},
  origpublisher = {Editions Robert Laffont},
  origtitle = {Histoire universelle des chiffres},
  pagetotal = {410},
  keywords = {computer science,exact sciences,history,human sciences},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ifrah - 2001 - The universal history of computing From the abacus to the quantum computer.pdf}
}

@article{ihaka1996,
  title = {R: {{A}} language for data analysis and graphics},
  shorttitle = {R},
  author = {Ihaka, Ross and Gentleman, Robert},
  date = {1996-09-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {3},
  pages = {299--314},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1080/10618600.1996.10474713},
  url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474713},
  urldate = {2025-02-24},
  abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
  langid = {english},
  keywords = {computer language,computer science,exact sciences,probability and statistics,r (programming language),statistical computing},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ihaka and Gentleman - 1996 - R A language for data analysis and graphics.pdf}
}

@book{landau,
  title = {The \{targets\} {{R}} package user manual},
  author = {Landau, Will},
  abstract = {Pipeline tools coordinate the pieces of computationally demanding analysis projects. The targets package is a Make-like pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.},
  langid = {english},
  keywords = {computer science,exact sciences,pipelines,programming,r (programming language),r packages}
}

@article{lohr2014,
  entrysubtype = {newspaper},
  title = {For big-data scientists, ‘{{Janitor}} work’ is key hurdle to insights},
  author = {Lohr, Steve},
  date = {2014-08-18},
  journaltitle = {The New York Times},
  location = {New York, NY},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html},
  urldate = {2024-06-10},
  abstract = {The analysis of giant data sets promises unique business insights, but much manual effort is still required to prepare the information for parsing.},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {data engineering,data science,exact sciences,probability and statistics}
}

@article{mariscal2021,
  title = {Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets},
  author = {Gómez-de-Mariscal, Estibaliz and Guerrero, Vanesa and Sneider, Alexandra and Jayatilaka, Hasini and Phillip, Jude M. and Wirtz, Denis and Muñoz-Barrutia, Arrate},
  date = {2021-10-22},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {20942},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-00199-5},
  url = {https://www.nature.com/articles/s41598-021-00199-5},
  urldate = {2023-11-26},
  abstract = {Biomedical research has come to rely on p-values as a deterministic measure for data-driven decision-making. In the largely extended null hypothesis significance testing for identifying statistically significant differences among groups of observations, a single p-value is computed from sample data. Then, it is routinely compared with a threshold, commonly set to 0.05, to assess the evidence against the hypothesis of having non-significant differences among groups, or the null hypothesis. Because the estimated p-value tends to decrease when the sample size is increased, applying this methodology to datasets with large sample sizes results in the rejection of the null hypothesis, making it not meaningful in this specific situation. We propose a new approach to detect differences based on the dependence of the p-value on the sample size. We introduce new descriptive parameters that overcome the effect of the size in the p-value interpretation in the framework of datasets with large sample sizes, reducing the uncertainty in the decision about the existence of biological differences between the compared experiments. The methodology enables the graphical and quantitative characterization of the differences between the compared experiments guiding the researchers in the decision process. An in-depth study of the methodology is carried out on simulated and experimental data. Code availability at https://github.com/BIIG-UC3M/pMoSS.},
  issue = {1},
  langid = {english},
  keywords = {exact sciences,p-value,p-value problem,power analysis,probability and statistics,sampling},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Gomez-de-Mariscal - 2021 - Use of the p-values as a size-dependent function to address practical.pdf}
}

@article{marwick2018,
  title = {Packaging data analytical work reproducibly using {{R}} (and friends)},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1375986},
  url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375986},
  urldate = {2023-07-17},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  langid = {english},
  keywords = {data science,exact sciences,programming,r (programming language),r packages},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Marwick - 2018 - Packaging data analytical work reproducibly using R (and friends).pdf}
}

@article{meyer2018,
  title = {Practical tips for ethical data sharing},
  author = {Meyer, Michelle N.},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {131--144},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245917747656},
  url = {https://doi.org/10.1177/2515245917747656},
  urldate = {2024-06-18},
  abstract = {This Tutorial provides practical dos and don’ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say—and what not to say—in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing “public” data.},
  langid = {english},
  keywords = {ethics,open data,open science,science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Meyer - 2018 - Practical tips for ethical data sharing.pdf}
}

@book{peng2022,
  title = {R programming for data science},
  author = {Peng, Roger D.},
  date = {2022-05-31},
  url = {https://bookdown.org/rdpeng/rprogdatascience/},
  urldate = {2025-02-25},
  abstract = {The R programming language has become the de facto programming language for data science. Its flexibility, power, sophistication, and expressiveness have made it an invaluable tool for data scientists around the world. This book is about the fundamentals of R programming. You will get started with the basics of the language, learn how to manipulate datasets, how to write functions, and how to debug and optimize code. With the fundamentals provided in this book, you will have a solid foundation on which to build your data science toolbox.},
  langid = {english},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,programming languages,r (programming language),textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Peng - 2022 - R programming for data science.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Peng - 2022 - R programming for data science.pdf}
}

@book{reis2022,
  title = {Fundamentals of data engineering: plan and build robust data systems},
  shorttitle = {Fundamentals of data engineering},
  author = {Reis, Joe and Housley, Matt},
  date = {2022},
  publisher = {O'Reilly},
  location = {Sebastopol, CA},
  isbn = {978-1-0981-0830-4},
  langid = {english},
  keywords = {data engineering,engineering,exact sciences,fundamentals of data engineering,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.pdf}
}

@article{rowley2007,
  title = {The wisdom hierarchy: representations of the {{DIKW}} hierarchy},
  shorttitle = {The wisdom hierarchy},
  author = {Rowley, Jennifer},
  date = {2007-04-01},
  journaltitle = {Journal of Information Science},
  volume = {33},
  number = {2},
  pages = {163--180},
  publisher = {SAGE Publications Ltd},
  issn = {0165-5515},
  doi = {10.1177/0165551506070706},
  url = {https://doi.org/10.1177/0165551506070706},
  urldate = {2024-06-10},
  abstract = {This paper revisits the data-information-knowledge-wisdom (DIKW) hierarchy by examining the articulation of the hierarchy in a number of widely read textbooks, and analysing their statements about the nature of data, information, knowledge, and wisdom. The hierarchy referred to variously as the ‘Knowledge Hierarchy’, the ‘Information Hierarchy’ and the ‘Knowledge Pyramid’ is one of the fundamental, widely recognized and ‘taken-for-granted’ models in the information and knowledge literatures. It is often quoted, or used implicitly, in definitions of data, information and knowledge in the information management, information systems and knowledge management literatures, but there has been limited direct discussion of the hierarchy. After revisiting Ackoff’s original articulation of the hierarchy, definitions of data, information, knowledge and wisdom as articulated in recent textbooks in information systems and knowledge management are reviewed and assessed, in pursuit of a consensus on definitions and transformation processes. This process brings to the surface the extent of agreement and dissent in relation to these definitions, and provides a basis for a discussion as to whether these articulations present an adequate distinction between data, information, and knowledge. Typically information is defined in terms of data, knowledge in terms of information, and wisdom in terms of knowledge, but there is less consensus in the description of the processes that transform elements lower in the hierarchy into those above them, leading to a lack of definitional clarity. In addition, there is limited reference to wisdom in these texts.},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Rowley - 2007 - The wisdom hierarchy.pdf}
}

@article{turing1937,
  title = {On computable numbers, with an application to the entscheidungsproblem},
  author = {Turing, A. M.},
  date = {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  shortjournal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  issn = {00246115},
  doi = {10.1112/plms/s2-42.1.230},
  url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {computer science,exact sciences,historical publications,máquina computadora universal},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Turing - 1937 - On computable numbers, with an application to the entscheidungsproblem.pdf}
}

@book{vanderloo2018,
  title = {Statistical data cleaning with applications in {{R}}},
  author = {family=Loo, given=Mark, prefix=van der, useprefix=true and family=Jonge, given=Edwin, prefix=de, useprefix=false},
  date = {2018},
  publisher = {John Wiley \& Sons},
  location = {Hoboken, NJ},
  abstract = {A comprehensive guide to automated statistical data cleaning  The production of clean data is a complex and time-consuming process that requires both technical know-how and statistical expertise. Statistical Data Cleaning brings together a wide range of techniques for cleaning textual, numeric or categorical data. This book examines technical data cleaning methods relating to data representation and data structure. A prominent role is given to statistical data validation, data cleaning based on predefined restrictions, and data cleaning strategy. Key features: - Focuses on the automation of data cleaning methods, including both theory and applications written in R.   - Enables the reader to design data cleaning processes for either one-off analytical purposes or for setting up production systems that clean data on a regular basis.   - Explores statistical techniques for solving issues such as incompleteness, contradictions and outliers, integration of data cleaning components and quality monitoring.   - Supported by an accompanying website featuring data and R code. This book enables data scientists and statistical analysts working with data to deepen their understanding of data cleaning as well as to upgrade their practical data cleaning skills. It can also be used as material for a course in data cleaning and analyses.},
  isbn = {978-1-118-89714-0},
  langid = {english},
  pagetotal = {300},
  keywords = {computer science,data munging,data science,exact sciences,probability and statistics,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Jonge - 2018 - Statistical data cleaning with applications in R.pdf}
}

@article{vonneumann1993a,
  title = {First draft of a report on the {{EDVAC}}},
  author = {family=Neumann, given=John, prefix=von, useprefix=true},
  date = {1993},
  journaltitle = {IEEE Annals of the History of Computing},
  volume = {15},
  number = {4},
  pages = {27--75},
  issn = {1934-1547},
  doi = {10.1109/85.238389},
  url = {https://ieeexplore.ieee.org/document/238389},
  urldate = {2025-02-24},
  abstract = {The first draft of a report on the EDVAC written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine (ACE) as the definitive source for understanding the nature and design of a general-purpose digital computer.},
  eventtitle = {{{IEEE Annals}} of the {{History}} of {{Computing}}},
  langid = {english},
  keywords = {computer science,electrical engineering,engines,exact sciences,extraordinary publications,forward contracts,history,laboratories,mathematics,pain,physics computing,proposals,statistics,stored-program concept},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/von Neumann - 1993 - First draft of a report on the EDVAC 2.pdf}
}

@article{wickham2014,
  title = {Tidy data},
  author = {Wickham, Hadley},
  date = {2014-09-12},
  journaltitle = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  url = {https://doi.org/10.18637/jss.v059.i10},
  urldate = {2024-06-08},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  langid = {english},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/XIK5UPP8/Wickham - 2014 - Tidy data.pdf}
}

@book{wickham2016a,
  title = {ggplot2: {{Elegant}} graphics for data analysis},
  shorttitle = {ggplot2},
  author = {Wickham, Hadley},
  namea = {Sievert, Carson},
  nameatype = {collaborator},
  date = {2016-06-16},
  series = {Use {{R}}!},
  edition = {2},
  publisher = {Springer},
  location = {Cham, Switzerland},
  doi = {10.1007/978-3-319-24277-4},
  url = {https://ggplot2-book.org},
  abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to:produce handsome, publication-quality plots with automatic legends created from the plot specificationsuperimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scalesadd customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plotsapproach a graph from a visual perspective, thinking about how each component of the data is represented on the final plotThis book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R).~ ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
  isbn = {978-3-319-24277-4},
  langid = {english},
  pagetotal = {260},
  keywords = {computer science,data science,data visualization,ggplot2,grammar of graphics,probability and statistics,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2016 - ggplot2 Elegant graphics for data analysis.pdf}
}

@book{wickham2019,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  date = {2019-05-30},
  edition = {2},
  publisher = {CRC Press},
  location = {Boca Raton},
  url = {https://adv-r.hadley.nz},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimising your code. By reading this book, you will learn: - The difference between an object and its name, and why the distinction is important - The important vector data structures, how they fit together, and how you can pull them apart using subsetting - The fine details of functions and environments - The condition system, which powers messages, warnings, and errors - The powerful functional programming paradigm, which can replace many for loops - The three most important OO systems: S3, S4, and R6 - The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation - Effective debugging techniques that you can deploy, regardless of how your code is run - How to find and remove performance bottlenecks The second edition is a comprehensive update: - New foundational chapters: "Names and values," "Control flow," and "Conditions" - Comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them - Much deeper coverage of metaprogramming, including the new tidy evaluation framework - Use of new package like rlang (rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (purrr.tidyverse.org/) for functional programming - Use of color in code chunks and figures},
  isbn = {978-0-8153-8457-1},
  langid = {english},
  pagetotal = {588},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@book{wickham2023d,
  title = {R packages},
  author = {Wickham, Hadley and Bryan, Jennifer},
  date = {2023},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r-pkgs.org},
  urldate = {2024-07-07},
  abstract = {Turn your R code into packages that others can easily install and use. With this fully updated edition, developers and data scientists will learn how to bundle reusable R functions, sample data, and documentation together by applying the package development philosophy used by the team that maintains the "tidyverse" suite of packages. In the process, you'll learn how to automate common development tasks using a set of R packages, including devtools, usethis, testthat, and roxygen2. Authors Hadley Wickham and Jennifer Bryan from Posit (formerly known as RStudio) help you create packages quickly, then teach you how to get better over time. You'll be able to focus on what you want your package to do as you progressively develop greater mastery of the structure of a package.},
  isbn = {978-1-0981-3494-5},
  langid = {english},
  pagetotal = {381},
  keywords = {data science,engineering,exact sciences,programming,r (programming language),r packages,software engineering},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.pdf}
}

@book{wickham2023e,
  title = {R for data science: {{Import}}, tidy, transform, visualize, and model data},
  shorttitle = {R for data science},
  author = {Wickham, Hadley and Çetinkaya-Rundel, Mine and Grolemund, Garrett},
  date = {2023-07-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r4ds.hadley.nz},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverseâ??a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way.},
  isbn = {978-1-4920-9740-2},
  langid = {american},
  pagetotal = {576},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.pdf}
}

@book{wickhamb,
  title = {The tidyverse style guide},
  author = {Wickham, Hadley},
  url = {https://style.tidyverse.org},
  urldate = {2023-07-17},
  langid = {english},
  keywords = {computer science,exact sciences,guia de estilo,nosource,r (programming language),standards}
}

@book{wilkinson2005,
  title = {The grammar of graphics},
  author = {Wilkinson, Leland},
  editor = {Chambers, J. and Hand, D. and Härdle, W.},
  editortype = {redactor},
  namea = {Wills, Graham and Rope, Dan and Norton, Andrew and Dubbs, Roger},
  nameatype = {collaborator},
  date = {2005},
  series = {Statistics and {{Computing}}},
  edition = {2},
  publisher = {Springer},
  location = {New York, NY},
  abstract = {Before writing the graphics for SYSTAT in the 1980’s, I began by teaching a seminar in statistical graphics and collecting as many different quantitative graphics as I could find. I was determined to produce a package that could draw every statistical graphic I had ever seen. The structure of the program was a collection of procedures named after the basic graph types they p- duced. The graphics code was roughly one and a half megabytes in size. In the early 1990’s, I redesigned the SYSTAT graphics package using - ject-based technology. I intended to produce a more comprehensive and - namic package. I accomplished this by embedding graphical elements in a tree structure. Rendering graphics was done by walking the tree and editing worked by adding and deleting nodes. The code size fell to under a megabyte. In the late 1990’s, I collaborated with Dan Rope at the Bureau of Labor Statistics and Dan Carr at George Mason University to produce a graphics p- duction library called GPL, this time in Java. Our goal was to develop graphics components. This book was nourished by that project. So far, the GPL code size is under half a megabyte.},
  isbn = {978-0-387-24544-7},
  langid = {english},
  pagetotal = {690},
  keywords = {computer science,data science,data visualization,design,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/TRDYALYB/Wilkinson - 2005 - The grammar of graphics.pdf}
}

@article{wilkinson2016,
  title = {The {{FAIR}} guiding principles for scientific data management and stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and Da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’T Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and Van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and Van Der Lei, Johan and Van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2023-07-17},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {data management,data science,exact sciences,open data,open science,princípios fair,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wilkinson - 2016 - The FAIR guiding principles for scientific data management and stewardship.pdf}
}
