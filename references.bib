@article{ackoff1989,
  title = {From data to wisdom},
  author = {Ackoff, Russell},
  date = {1989},
  journaltitle = {Journal of Applied Systems Analysis},
  volume = {16},
  pages = {3--9},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ackoff - 1989 - From data to wisdom.pdf}
}

@article{bernardes2009a,
  title = {Ensino e aprendizagem como unidade dialética na atividade pedagógica},
  author = {Bernardes, Maria Eliza Mattosinho},
  date = {2009-12},
  journaltitle = {Psicologia Escolar e Educacional},
  shortjournal = {Psicol. Esc. Educ.},
  volume = {13},
  number = {2},
  pages = {235--242},
  issn = {1413-8557},
  doi = {10.1590/S1413-85572009000200005},
  url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1413-85572009000200005&lng=pt&tlng=pt},
  urldate = {2023-07-17},
  abstract = {Considerar a atividade pedagógica como unidade dialética requer que sejam investigadas as condições e circunstâncias que tornam possível tal objetivação e as especificidades da mesma a partir do materialismo histórico dialético, da psicologia histórico-cultural e da teoria da atividade. Concebe-se que a correspondência entre motivos/necessidades, objetivos, ações e operações nas atividades principais dos estudantes e dos educadores cria situações favoráveis para que ocorra a aprendizagem consciente. Tal fato é dependente da consciência da função ontológica do educador e do estudante como sujeitos ativos no processo de ensino-aprendizagem. Considera-se ser possível conceber a unidade dialética desde que a mesma represente uma das condições para a superação das características do sistema educacional vigente, por caracterizar-se a atividade material do homem como a que transforma o mundo material e social em mundo humano.},
  langid = {portuguese},
  keywords = {education,human sciences,pedagogia,preparação pedagógica},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Bernardes - 2009 - Ensino e aprendizagem como unidade dialetica na atividade pedagogica.pdf}
}

@online{broman2013,
  title = {Data science is statistics},
  author = {Broman, Karl},
  date = {2013-04-05T13:12:35+00:00},
  url = {https://kbroman.wordpress.com/2013/04/05/data-science-is-statistics/},
  urldate = {2024-06-10},
  langid = {english},
  organization = {The stupidest thing...},
  keywords = {data science,exact sciences,probability and statistics}
}

@book{brookshear2020,
  title = {Computer science: {{An}} overview},
  shorttitle = {Computer science},
  author = {Brookshear, J. Glenn and Brylow, Dennis},
  date = {2020},
  edition = {13 ed., Global edition},
  publisher = {Pearson},
  location = {Harlow, UK},
  abstract = {Computer Science: An Overview, 13th edition, Global Edition, by J. Glenn Brookshear, and Dennis Brylow, is written for students from all backgrounds, giving you a bottom-up, concrete-to-abstract foundation in the subject. Its broad coverage encourages a practical and realistic understanding of computer science, covering all the major concepts. The book's broad background exposes beginning computer science students to the breadth of the subject they plan to major in and teaches students from other backgrounds how to relate to the technical society in which they live. Learn in a flexible way with independent chapters you can study in any order with full-colour design to help you engage with the information. The text also uses Python to provide programming tools for exploration and experimentation in your learning. This 13th edition has been corrected and updated in each chapter to refine your learning experience. With more than 1,000 questions and exercises, the book trains your thinking skills with useful chapter review problems and contains questions surrounding social issues to reinforce core concepts. This text is comprehensive and highly accessible, making it ideal for undergraduate studies in computer science.},
  isbn = {978-1-292-26344-1},
  langid = {english},
  pagetotal = {735},
  keywords = {computer science,exact sciences,fundamentals of computer science,textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Brookshear - 2020 - Computer science.pdf}
}

@book{bryan,
  title = {Happy {{Git}} and {{GitHub}} for the {{useR}}},
  author = {Bryan, Jennifer},
  url = {https://happygitwithr.com/},
  abstract = {Happy Git provides opinionated instructions on how to: - Install Git and get it working smoothly with GitHub, in the shell and in the RStudio IDE. - Develop a few key workflows that cover your most common tasks. - Integrate Git and GitHub into your daily work with R and R Markdown. The target reader is someone who uses R for data analysis or who works on R packages, although some of the content may be useful to those working in adjacent areas.},
  langid = {american},
  keywords = {computer science,r (programming language),r (programming language) fundamentals}
}

@book{bryana,
  title = {What they forgot to teach you about {{R}}: the stuff you need to know about {{R}}, besides data analysis},
  shorttitle = {What they forgot to teach you about {{R}}},
  author = {Bryan, Jennifer and Hester, Jim and Pileggi, Shannon and Aja, E. David},
  url = {https://rstats.wtf/},
  abstract = {This book focuses on content intrinsically related to the infrastructure surrounding data analysis in R, but does not delve into the data analysis itself. - *A holistic workflow* provides guidance on project-oriented workflows that address common sources of friction in data analysis. - *Personal R administration* empowers R users to confidently manage their R programming environment. - *All is Fail* showcases functions, options, and RStudio capabilities for debugging code, facilitating more efficient resolution of errant code.},
  langid = {american},
  keywords = {computer science,r (programming language),r (programming language) fundamentals}
}

@article{cao2017,
  title = {Data science: a comprehensive overview},
  shorttitle = {Data science},
  author = {Cao, Longbing},
  date = {2017-06-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {3},
  pages = {43},
  issn = {0360-0300},
  doi = {10.1145/3076253},
  url = {https://dl.acm.org/doi/10.1145/3076253},
  urldate = {2024-06-10},
  abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/U4EIKQ7Q/Cao - 2017 - Data Science A Comprehensive Overview.pdf}
}

@article{carvalho2021a,
  title = {Measuring sustainable food systems in {{Brazil}}: a framework and multidimensional index to evaluate socioeconomic, nutritional, and environmental aspects},
  shorttitle = {Measuring sustainable food systems in {{Brazil}}},
  author = {family=Carvalho, given=Aline Martins, prefix=de, useprefix=false and Verly Jr, Eliseu and Marchioni, Dirce Maria and Jones, Andrew D.},
  date = {2021-07-01},
  journaltitle = {World Development},
  shortjournal = {World Development},
  volume = {143},
  pages = {105470},
  issn = {0305-750X},
  doi = {10.1016/j.worlddev.2021.105470},
  url = {https://www.sciencedirect.com/science/article/pii/S0305750X21000826},
  urldate = {2024-07-05},
  abstract = {The sustainability of food systems is commonly measured at the global or national level using multidimensional indices. However, it is not possible to use such indices at subnational levels, because the data for many indicators are not available and large countries such as Brazil have completely different food systems depending on the state or region considered. This paper presents a sustainable food systems framework for Brazil and the Brazilian Multidimensional Index for Sustainable Food Systems (MISFS), which is the first index of its kind that considers local behaviors and actions to rank states and regions of the country. The dimensions and indicators included in the index were identified from a literature review and an exclusion criteria, which considered the Brazilian context, that ended up with 17 indicators divided in three dimensions (environmental, socioeconomic, and nutritional). We followed well-known procedures to create an interpretable multidimensional index, which include normalization, weighting, aggregating, and sensitivity analysis. The performance of the states was starkly different, regarding the indicators and dimensions considered. No state presented a high score for all three dimensions, and even states with a high MISFS score had room to improve in one or two dimensions. In general, high-income states scored well on socioeconomic indicators (e.g., agriculture education), but poorly on most environmental indicators (e.g., water footprint), and some nutritional indicators (e.g., obesity rate). This index is the first initiative to measure the Brazilian food systems according to local practices and behaviors at state level and it is a starting point to help policymakers understand the opportunities of improvement of their food systems. This study may also help other countries to develop their own frameworks and strategies to improve their regionalized food systems.},
  keywords = {brazil,environmental sciences,exact sciences,health sciences,índice,misfs,nutrition},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Carvalho - 2021 - Measuring sustainable food systems in Brazil.pdf}
}

@book{casella2002,
  title = {Statistical inference},
  author = {Casella, George and Berger, Roger L.},
  date = {2002},
  series = {Duxbury advanced series},
  edition = {2},
  publisher = {Duxbury},
  location = {Pacific Grove, CA},
  isbn = {0-534-24312-6},
  langid = {english},
  pagetotal = {660},
  keywords = {exact sciences,fundamentals of probability and statistics,probability and statistics,statistical inference,textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Casella - 2002 - Statistical inference.pdf}
}

@book{chang2018,
  title = {R graphics cookbook: {{Practical}} recipes for visualizing data},
  shorttitle = {R graphics cookbook},
  author = {Chang, Winston},
  date = {2018-12-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Beijing Boston Farnham},
  url = {https://r-graphics.org},
  abstract = {This O’Reilly cookbook provides more than 150 recipes to help scientists, engineers, programmers, and data analysts generate high-quality graphs quickly―without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project and includes a discussion of how and why the recipe works.  Most of the recipes in this second edition use the updated version of the ggplot2 package, a powerful and flexible way to make graphs in R. You’ll also find expanded content about the visual design of graphics. If you have at least a basic understanding of the R language, you’re ready to get started with this easy-to-use reference. Use R’s default graphics for quick exploration of data Create a variety of bar graphs, line graphs, and scatter plots Summarize data distributions with histograms, density curves, box plots, and more Provide annotations to help viewers interpret data Control the overall appearance of graphics Explore options for using colors in plots Create network graphs, heat maps, and 3D scatter plots Get your data into shape using packages from the tidyverse},
  isbn = {978-1-4919-7860-3},
  langid = {english},
  pagetotal = {441},
  keywords = {computer science,data science,data visualization,design,exact sciences,ggplot2,grammar of graphics,r (programming language),textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Chang - 2018 - R graphics cookbook Practical recipes for visualizing data.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Chang - 2018 - R graphics cookbook Practical recipes for visualizing data.pdf}
}

@video{code.org2018,
  entrysubtype = {video},
  title = {How computers work},
  editor = {{Code.org}},
  editortype = {director},
  date = {2018-01-30},
  url = {https://youtube.com/playlist?list=PLzdnOPI1iJNcsRwJhvksEo1tJqjIqWbN-&si=WkuM8c-AKI-NZ3td},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {computer science,fundamentals of computer science}
}

@book{cohen1988a,
  title = {Statistical power analysis for the behavioral sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2},
  publisher = {Lawrence Erlbaum Associates},
  location = {Hillsdale, N.J},
  isbn = {978-0-8058-0283-2},
  pagetotal = {567},
  keywords = {effect size,exact sciences,power analysis,probability and statistics,sampling,textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Cohen - 1988 - Statistical power analysis for the behavioral sciences.pdf}
}

@book{coronel2019,
  title = {Database systems: design, implementation, and management},
  shorttitle = {Database systems},
  author = {Coronel, Carlos and Morris, Steven A.},
  date = {2019},
  edition = {13},
  publisher = {Cengage},
  location = {Boston, MA},
  isbn = {978-1-337-62790-0},
  langid = {english},
  pagetotal = {802},
  keywords = {computer science,databases,exact sciences,information science,modeling,relational databases},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Coronel - 2019 - Database systems.pdf}
}

@book{degroot2012a,
  title = {Probability and statistics},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  date = {2012},
  edition = {4},
  publisher = {Addison-Wesley},
  location = {Boston},
  isbn = {978-0-321-50046-5},
  langid = {english},
  pagetotal = {893},
  keywords = {exact sciences,fundamentals of probability and statistics,probability and statistics,textbooks},
  annotation = {OCLC: ocn502674206},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/DeGroot - 2012 - Probability and statistics.pdf;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/DeGroot - 2012 - Probability and statistics.zip}
}

@article{denning2005,
  title = {Is computer science science?},
  author = {Denning, Peter J.},
  date = {2005-04-01},
  journaltitle = {Communications of the ACM},
  volume = {48},
  number = {4},
  pages = {27--31},
  issn = {0001-0782},
  doi = {10.1145/1053291.1053309},
  url = {https://dl.acm.org/doi/10.1145/1053291.1053309},
  urldate = {2024-08-04},
  abstract = {Computer science meets every criterion for being a science, but it has a self-inflicted credibility problem.},
  langid = {english},
  keywords = {computer science,exact sciences,perspectives},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Denning - 2005 - Is computer science science.pdf}
}

@article{dhar2023,
  title = {Data science and prediction},
  author = {Dhar, Vasant},
  date = {2023-12-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {64--73},
  issn = {0001-0782},
  doi = {10.1145/2500499},
  url = {https://dl.acm.org/doi/10.1145/2500499},
  urldate = {2024-06-10},
  abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Dhar - 2023 - Data science and prediction.pdf}
}

@book{ellis2010,
  title = {The essential guide to effect sizes: statistical power, meta-analysis, and the interpretation of research results},
  shorttitle = {The essential guide to effect sizes},
  editor = {Ellis, Paul D.},
  date = {2010},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK},
  abstract = {This succinct and jargon-free introduction to effect sizes gives students and researchers the tools they need to interpret the practical significance of their results. Using a class-tested approach that includes numerous examples and step-by-step exercises, it introduces and explains three of the most important issues relating to the practical significance of research results: the reporting and interpretation of effect sizes (Part I), the analysis of statistical power (Part II), and the meta-analytic pooling of effect size estimates drawn from different studies (Part III). The book concludes with a handy list of recommendations for those actively engaged in or currently preparing research projects.},
  isbn = {978-0-521-19423-5},
  langid = {english},
  pagetotal = {173},
  keywords = {effect size,exact sciences,probability and statistics,textbooks},
  annotation = {a},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ellis - 2010 - The essential guide to effect sizes statistical power, meta-analysis, and the interpretation of res.pdf}
}

@article{ellis2018,
  title = {How to share data for collaboration},
  author = {Ellis, Shannon E. and Leek, Jeffrey T.},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  eprint = {32981941},
  eprinttype = {pubmed},
  pages = {53--57},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375987},
  url = {https://doi.org/10.1080/00031305.2017.1375987},
  urldate = {2024-06-17},
  abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
  keywords = {data science,exact sciences,interdisciplinary fields,probability and statistics,tidy data},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ellis - 2018 - How to share data for collaboration.pdf}
}

@book{engels2020a,
  title = {Dialética da natureza},
  author = {Engels, Friedrich},
  translator = {Schneider, Nélio},
  date = {2020},
  publisher = {Boitempo},
  location = {São Paulo},
  isbn = {978-65-5717-024-3},
  langid = {brazilian},
  keywords = {biological sciences,complexity science,dialectical materialism,dialectics,human sciences,interdisciplinary fields,marxism,materialism,sociology},
  annotation = {Traduzido do original em alemão Dialektik der natur (1873-1882).},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Engels - 2020 - Dialetica da natureza.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Engels - 2020 - Dialetica da natureza.pdf}
}

@book{freire2011,
  title = {Pedagogia da autonomia: saberes necessários à prática educativa},
  shorttitle = {Pedagogia Da Autonomia},
  author = {Freire, Paulo},
  date = {2011},
  publisher = {Paz e Terra},
  location = {São Paulo},
  isbn = {978-85-7753-226-1},
  langid = {portuguese},
  keywords = {education,human sciences,paulo freire,pedagogia,preparação pedagógica},
  annotation = {OCLC: 1229931361},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Freire - 2011 - Pedagogia da autonomia.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Freire - 2011 - Pedagogia da autonomia.pdf}
}

@online{gavin2020,
  title = {Big data will be dead in 5 years},
  author = {Gavin, Lewis},
  date = {2020-10-20},
  url = {https://towardsdatascience.com/big-data-will-be-dead-in-5-years-ef4344269aef},
  urldate = {2024-06-10},
  abstract = {Everything will be big data, so it won't need a special name.},
  langid = {english},
  organization = {Towards Data Science},
  keywords = {big data,data science,exact sciences,probability and statistics}
}

@article{gigerenzer2004,
  title = {Mindless statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11-01},
  journaltitle = {The Journal of Socio-Economics},
  shortjournal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
  urldate = {2024-10-04},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {collective illusions,exact sciences,null hypothesis significance testing,null ritual,probability and statistics,rituals,statistical rituals,statistical significance,statistical thinking,textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Gigerenzer - 2004 - Mindless statistics.pdf}
}

@online{gofairinitiative,
  title = {{{GO FAIR}} initiative: make your data \& services {{FAIR}}},
  shorttitle = {{{GO FAIR}} initiative},
  author = {{GO FAIR initiative}},
  url = {https://www.go-fair.org/},
  urldate = {2024-06-10},
  abstract = {A bottom-up international approach for the practical implementation of the European Open Science Cloud (EOSC) as part of a global Internet of FAIR Data \& Services},
  langid = {american},
  organization = {GO FAIR},
  keywords = {data engineering,open data,open science,science}
}

@online{greener2020,
  title = {Stop testing for normality},
  author = {Greener, Robert},
  date = {2020-08-04T12:53:26},
  url = {https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90},
  urldate = {2024-09-29},
  abstract = {Normality tests are misleading and a waste of your time!},
  langid = {english},
  organization = {Medium},
  keywords = {exact sciences,general linear models,normality tests,tests},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Greener - 2020 - Stop testing for normality.pdf}
}

@book{grosser2021,
  title = {Advanced {{R}} solutions},
  author = {Grosser, Malte and Bumann, Henning and Wickham, Hadley},
  date = {2021-08-24},
  publisher = {CRC Press},
  location = {Boca Raton, FL},
  abstract = {This book offers solutions to all 284 exercises in Advanced R, Second Edition. All the solutions have been carefully documented and made to be as clear and accessible as possible. Working through the exercises and their solutions will give you a deeper understanding of a variety of programming challenges, many of which are relevant to everyday work. This will expand your set of tools on a technical and conceptual level. You will be able to transfer many of the specific programming schemes directly and will discover far more elegant solutions to everyday problems.Features:When R creates copies, and how it affects memory usage and code performanceEverything you could ever want to know about functionsThe differences between calling and exiting handlersHow to employ functional programming to solve modular tasksThe motivation, mechanics, usage, and limitations of R's highly pragmatic S3 OO systemThe R6 OO system, which is more like OO programming in other languagesThe rules that R uses to parse and evaluate expressionsHow to use metaprogramming to generate HTML or LaTeX with elegant R codeHow to identify and resolve performance bottlenecks},
  isbn = {978-1-032-00749-6},
  langid = {english},
  pagetotal = {302},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@article{horne1976,
  title = {A self-assessment questionnaire to determine morningness-eveningness in human circadian rhythms},
  author = {Horne, J. A. and Östberg, O.},
  date = {1976},
  journaltitle = {International Journal of Chronobiology},
  shortjournal = {Int J Chronobiol},
  volume = {4},
  number = {2},
  eprint = {1027738},
  eprinttype = {pubmed},
  pages = {97--110},
  issn = {0300-9998},
  abstract = {An English language self-assessment Morningness-Eveningness questionnaire is presented and evaluated against individual differences in the circadian vatiation of oral temperature. 48 subjects falling into Morning, Evening and Intermediate type categories regularly took their temperature. Circadian peak time were identified from the smoothed temperature curves of each subject. Results showed that Morning types and a significantly earlier peak time than Evening types and tended to have a higher daytime temperature and lower post peak temperature. The Intermediate type had temperatures between those of the other groups. Although no significant differences in sleep lengths were found between the three types, Morning types retired and arose significantly earlier than Evening types. Whilst these time significatly correlated with peak time, the questionnaire showed a higher peak time correlation. Although sleep habits are an important déterminant of peak time there are other contibutory factors, and these appear to be partly covered by the questionnaire. Although the questionnaire appears to be valid, further evaluation using a wider subject population is required.},
  langid = {english},
  keywords = {biological sciences,chronobiology,fundamentals of chronobiology,meq},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Horne - 1976 - A self-assessment questionnaire to determine morningness-eveningness in human.pdf}
}

@book{ifrah2001,
  title = {The universal history of computing: {{From}} the abacus to the quantum computer},
  shorttitle = {The universal history of computing},
  author = {Ifrah, Georges},
  translator = {Harding, E. F.},
  namea = {{Sophie Wood} and {Ian Monk} and {Elizabeth Clegg} and {Guido Waldman}},
  nameatype = {collaborator},
  date = {2001},
  origdate = {1994},
  publisher = {John Wiley \& Sons},
  location = {New York, NY},
  abstract = {Suppose every instrument could by command or by anticipation of need execute its function on its own; suppose that spindles could weave of their own accord, and plectra strike the strings of zithers by themselves; then craftsmen would have no need of hand-work, and masters have no need of slaves." Aristotle Called the Indiana Jones of arithmetic, Georges Ifrah embarked in 1974 on a ten-year quest to discover where numbers come from and what they say about us. His first book, the highly praised Universal History of Numbers, drew from this remarkable journey, presented the first complete account of the invention and evolution of numbers the world overand became an international bestseller. In The Universal History of Computing, Ifrah continues his exhilarating exploration into the fascinating world of numbers. In this fun, engaging but no less learned book, he traces the development of computing from the invention of the abacus to the creation of the binary system three centuries ago to the incredible conceptual, scientific, and technical achievements that made the first modern computers possible. He shows us how various cultures, scientists, and industries across the world struggled to break free of the tedious labor of mental calculation and, as a result, he reveals the evolution of the human mind. Evoking the excitement and joy that accompanied the grand mathematical undertakings throughout history, Ifrah takes us along as he revisits a multitude of cultures, from Roman times and the Chinese Common Era to twentieth-century England and America. We meet mathematicians, visionaries, philosophers, and scholars from every corner of the world and from every period of history. We witness the dead ends and regressions in the computers development, as well as the advances and illuminating discoveries. We learn about the births of the pocket calculator, the adding machine, the cash register, and even automata. We find out how the origins of the computer can be found in the European Renaissance, along with how World War II influenced the development of analytical calculation. And we explore such hot topics as numerical codes and the recent discovery of new kinds of number systems, such as "surreal" numbers. Adventurous and enthralling, The Universal History of Computing is an astonishing achievement that not only unravels the epic tale of computing, but also tells the compelling story of human intelligenceand how much farther we still have to go.},
  isbn = {0-471-39671-0},
  langid = {english},
  origlanguage = {fr},
  origlocation = {Paris},
  origpublisher = {Editions Robert Laffont},
  origtitle = {Histoire universelle des chiffres},
  pagetotal = {410},
  keywords = {computer science,exact sciences,history,human sciences},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ifrah - 2001 - The universal history of computing From the abacus to the quantum computer.pdf}
}

@article{ihaka1996,
  title = {R: {{A}} language for data analysis and graphics},
  shorttitle = {R},
  author = {Ihaka, Ross and Gentleman, Robert},
  date = {1996-09-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {3},
  pages = {299--314},
  publisher = {ASA Website},
  issn = {1061-8600},
  doi = {10.1080/10618600.1996.10474713},
  url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474713},
  urldate = {2025-02-24},
  abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
  langid = {english},
  keywords = {computer language,computer science,exact sciences,probability and statistics,r (programming language),statistical computing},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ihaka and Gentleman - 1996 - R A language for data analysis and graphics.pdf}
}

@article{kozak2018,
  title = {What's normal anyway? {{Residual}} plots are more telling than significance tests when checking {{ANOVA}} assumptions},
  shorttitle = {What's normal anyway?},
  author = {Kozak, M. and Piepho, H.-P.},
  date = {2018},
  journaltitle = {Journal of Agronomy and Crop Science},
  volume = {204},
  number = {1},
  pages = {86--98},
  issn = {1439-037X},
  doi = {10.1111/jac.12220},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jac.12220},
  urldate = {2024-09-29},
  abstract = {We consider two questions important for applying analysis of variance (ANOVA): Should normality be checked on the raw data or on the residuals (or is it immaterial which of the two approaches we take)? Should normality and homogeneity of variance be checked using significance tests or diagnostic plots (or both)? Based on two examples, we show that residuals should be used for model checking and that residual plots are better for checking ANOVA assumptions than statistical tests. We also discuss why one should be very cautious when using statistical tests to check the assumptions.},
  langid = {english},
  keywords = {anova,assumption checks,diagnostic plots,exact sciences,general linear models,linear models,probability and statistics,statistical assumptions},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Kozak and Piepho - 2018 - What's normal anyway Residual plots are more telling than significance tests when checking ANOVA as.pdf}
}

@book{kuhn2022,
  title = {Tidy modeling with {{R}}: a framework for modeling in the tidyverse},
  shorttitle = {Tidy modeling with {{R}}},
  author = {Kuhn, Max and Silge, Julia},
  date = {2022},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://www.tmwr.org/},
  abstract = {Get going with tidymodels, a collection of R packges for modeling and machine learning. Whether you're just starting out or have years of experience with modeling, this practical introduction shows data analysts, business analysts, and data scientists how the tidymodels framework offers a consistent, flexible approach for your work. RStudio engineers Max Kuhn and Julia Silge demonstrate ways to create models by focusing on an R dialect called the tidyverse. Software that adops tidyverse principles shares both a high-level design philosophy and low-level grammar and data structures, so learning one piece of the ecosystem makes it easier to learn the next. You'll understand why the tidymodels framework has been built to be used by a broad range of people.},
  isbn = {978-1-4920-9648-1},
  langid = {english},
  pagetotal = {363},
  keywords = {artificial intelligence,exact sciences,machine learning,modeling,probability and statistics,programming,r (programming language),tidyverse},
  annotation = {OCLC: on1338675673},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Kuhn - 2022 - Tidy modeling with R.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Kuhn - 2022 - Tidy modeling with R.pdf}
}

@book{landau,
  title = {The \{targets\} {{R}} package user manual},
  author = {Landau, Will},
  abstract = {Pipeline tools coordinate the pieces of computationally demanding analysis projects. The targets package is a Make-like pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.},
  langid = {english},
  keywords = {computer science,exact sciences,pipelines,programming,r (programming language),r packages}
}

@article{leocadio-miguel2017,
  title = {Latitudinal cline of chronotype},
  author = {Leocadio-Miguel, Mario André and Louzada, Fernando Mazzili and Duarte, Leandro Lourenção and Areas, Roberta Peixoto and Alam, Marilene and Freire, Marcelo Ventura and Fontenele-Araujo, John and Menna-Barreto, Luiz and Pedrazzoli, Mario},
  date = {2017-07-14},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {7},
  number = {1},
  pages = {5437},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-05797-w},
  url = {https://www.nature.com/articles/s41598-017-05797-w},
  urldate = {2023-07-17},
  abstract = {Abstract             The rotation of the Earth around its own axis and around the sun determines the characteristics of the light/dark cycle, the most stable and ancient 24\,h temporal cue for all organisms. Due to the tilt in the earth’s axis in relation to the plane of the earth’s orbit around the sun, sunlight reaches the Earth differentially depending on the latitude. The timing of circadian rhythms varies among individuals of a given population and biological and environmental factors underlie this variability. In the present study, we tested the hypothesis that latitude is associated to the regulation of circadian rhythm in humans. We have studied chronotype profiles across latitudinal cline from around 0° to 32° South in Brazil in a sample of 12,884 volunteers living in the same time zone. The analysis of the results revealed that humans are sensitive to the different sunlight signals tied to differences in latitude, resulting in a morning to evening latitudinal cline of chronotypes towards higher latitudes.},
  langid = {english},
  keywords = {biological sciences,chronobiology,chronotype,entrainment,latitude,meq},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Leocadio-Miguel - 2017 - Latitudinal cline of chronotype.pdf}
}

@article{lohr2014,
  entrysubtype = {newspaper},
  title = {For big-data scientists, ‘{{Janitor}} work’ is key hurdle to insights},
  author = {Lohr, Steve},
  date = {2014-08-18},
  journaltitle = {The New York Times},
  location = {New York, NY},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html},
  urldate = {2024-06-10},
  abstract = {The analysis of giant data sets promises unique business insights, but much manual effort is still required to prepare the information for parsing.},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {data engineering,data science,exact sciences,probability and statistics}
}

@article{mariscal2021,
  title = {Use of the p-values as a size-dependent function to address practical differences when analyzing large datasets},
  author = {Gómez-de-Mariscal, Estibaliz and Guerrero, Vanesa and Sneider, Alexandra and Jayatilaka, Hasini and Phillip, Jude M. and Wirtz, Denis and Muñoz-Barrutia, Arrate},
  date = {2021-10-22},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {20942},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-00199-5},
  url = {https://www.nature.com/articles/s41598-021-00199-5},
  urldate = {2023-11-26},
  abstract = {Biomedical research has come to rely on p-values as a deterministic measure for data-driven decision-making. In the largely extended null hypothesis significance testing for identifying statistically significant differences among groups of observations, a single p-value is computed from sample data. Then, it is routinely compared with a threshold, commonly set to 0.05, to assess the evidence against the hypothesis of having non-significant differences among groups, or the null hypothesis. Because the estimated p-value tends to decrease when the sample size is increased, applying this methodology to datasets with large sample sizes results in the rejection of the null hypothesis, making it not meaningful in this specific situation. We propose a new approach to detect differences based on the dependence of the p-value on the sample size. We introduce new descriptive parameters that overcome the effect of the size in the p-value interpretation in the framework of datasets with large sample sizes, reducing the uncertainty in the decision about the existence of biological differences between the compared experiments. The methodology enables the graphical and quantitative characterization of the differences between the compared experiments guiding the researchers in the decision process. An in-depth study of the methodology is carried out on simulated and experimental data. Code availability at https://github.com/BIIG-UC3M/pMoSS.},
  issue = {1},
  langid = {english},
  keywords = {exact sciences,p-value,p-value problem,power analysis,probability and statistics,sampling},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Gomez-de-Mariscal - 2021 - Use of the p-values as a size-dependent function to address practical.pdf}
}

@article{marwick2018,
  title = {Packaging data analytical work reproducibly using {{R}} (and friends)},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1375986},
  url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375986},
  urldate = {2023-07-17},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  langid = {english},
  keywords = {data science,exact sciences,programming,r (programming language),r packages},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Marwick - 2018 - Packaging data analytical work reproducibly using R (and friends).pdf}
}

@article{mcilroy1978,
  title = {{{UNIX Time-Sharing System}}: {{Forward}}},
  author = {McIlroy, M. D. and Pinson, E. N. and Tague, B. A.},
  date = {1978},
  journaltitle = {Bell System Technical Journal},
  volume = {57},
  number = {6},
  pages = {1899--1904},
  url = {https://archive.org/details/bstj57-6-1899/mode/2up},
  langid = {english},
  keywords = {best practices,computer science,exact sciences,operating systems,unix,unix philosophy},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/McIlroy et al. - 1978 - UNIX Time-Sharing System Forward.pdf}
}

@article{meyer2018,
  title = {Practical tips for ethical data sharing},
  author = {Meyer, Michelle N.},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {131--144},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245917747656},
  url = {https://doi.org/10.1177/2515245917747656},
  urldate = {2024-06-18},
  abstract = {This Tutorial provides practical dos and don’ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say—and what not to say—in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing “public” data.},
  langid = {english},
  keywords = {ethics,open data,open science,science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Meyer - 2018 - Practical tips for ethical data sharing.pdf}
}

@article{najmi2021,
  title = {How to choose and interpret a statistical test? {{An}} update for budding researchers},
  shorttitle = {How to choose and interpret a statistical test?},
  author = {Najmi, Ahmad and Sadasivam, Balakrishnan and Ray, Avik},
  date = {2021-08-27},
  journaltitle = {Journal of Family Medicine and Primary Care},
  volume = {10},
  number = {8},
  pages = {2763},
  issn = {2249-4863},
  doi = {10.4103/jfmpc.jfmpc_433_21},
  url = {https://journals.lww.com/jfmpc/fulltext/2021/10080/how_to_choose_and_interpret_a_statistical_test__an.8.aspx},
  urldate = {2025-03-25},
  abstract = {Postgraduate medical students are often not able to select and interpret the findings of statistical tests during their thesis or research projects. To go ahead with selection of tests to be performed, researchers need to determine the objectives of study, types of variables, analysis and the study design, number of groups and data sets, and the types of distribution. In this review, we summarize and explain various statistical tests to help postgraduate medical students to select the most appropriate techniques for their thesis and dissertation.},
  langid = {american},
  keywords = {exact sciences,fundamentals of probability and statistics,hypothesis tests,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Najmi et al. - 2021 - How to choose and interpret a statistical test An update for budding researchers.pdf}
}

@article{neyman1928,
  title = {On the use and interpretation of certain test criteria for purposes of statistical inference: {{Part I}}},
  shorttitle = {On the use and interpretation of certain test criteria for purposes of statistical inference},
  author = {Neyman, J. and Pearson, E. S.},
  date = {1928},
  journaltitle = {Biometrika},
  volume = {20A},
  number = {1/2},
  eprint = {2331945},
  eprinttype = {jstor},
  pages = {175--240},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331945},
  url = {https://www.jstor.org/stable/2331945},
  urldate = {2024-09-30},
  langid = {english},
  keywords = {exact sciences,extraordinary publications,hypothesis tests,hypothetico–deductive method,probability and statistics,statistical inference},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Neyman and Pearson - 1928 - On the use and interpretation of certain test criteria for purposes of statistical inference part I.pdf}
}

@article{neyman1928a,
  title = {On the use and interpretation of certain test criteria for purposes of statistical inference: {{Part II}}},
  shorttitle = {On the use and interpretation of certain test criteria for purposes of statistical inference},
  author = {Neyman, J. and Pearson, E. S.},
  date = {1928},
  journaltitle = {Biometrika},
  volume = {20A},
  number = {3/4},
  eprint = {2332112},
  eprinttype = {jstor},
  pages = {263--294},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2332112},
  url = {https://www.jstor.org/stable/2332112},
  urldate = {2024-09-30},
  langid = {english},
  keywords = {exact sciences,extraordinary publications,hypothesis tests,hypothetico–deductive method,probability and statistics,statistical inference},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Neyman and Pearson - 1928 - On the use and interpretation of certain test criteria for purposes of statistical inference part I 1.pdf}
}

@article{norde2023,
  title = {Measuring food systems sustainability in heterogenous countries: {{The Brazilian}} multidimensional index updated version applicability},
  shorttitle = {Measuring food systems sustainability in heterogenous countries},
  author = {Norde, Marina Maintinguer and Porciuncula, Laura and Garrido, Giovanna and Nunes-Galbes, Nadine Marques and Sarti, Flavia Mori and Marchioni, Dirce Maria Lobo and family=Carvalho, given=Aline Martins, prefix=de, useprefix=true},
  date = {2023},
  journaltitle = {Sustainable Development},
  volume = {31},
  number = {1},
  pages = {91--107},
  issn = {1099-1719},
  doi = {10.1002/sd.2376},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sd.2376},
  urldate = {2025-01-21},
  abstract = {Countries with large territories marked by heterogeneity lack tools to monitor their distinct food systems sustainability. The multidimensional index for sustainable food systems (MISFS) was designed to measure food system sustainability locally, using the Brazilian territory. The aim was two-fold: to present the MISFS updating process (MISFS-R), and to show the applicability of MISFS-R to uncover local geopolitical priorities. Methods: Based on a systematized review and sensitivity analysis, 46 indicators were selected to compose the MISFS-R social, nutritional, environmental, and economic dimensions. Official Brazilian datasets allowed the MISFS-R scoring for 26 Brazilian states and capitals. Correlations between MISFS-R dimensions were analyzed by Spearman's coefficient. A cluster analysis was used to check for Brazilian states similarities and priorities. Findings: Economic and environmental performance were inversely related, while economic performance was directly related to social and nutritional scores. Four clusters were formed with marked differences in their food systems sustainability profile.},
  langid = {english},
  keywords = {brazil,economic,environmental,food system,health sciences,misfs,nutrition,nutritional,social,sustainability},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Norde et al. - 2023 - Measuring food systems sustainability in heterogenous countries The Brazilian multidimensional inde.pdf}
}

@book{peng2022,
  title = {R programming for data science},
  author = {Peng, Roger D.},
  date = {2022-05-31},
  url = {https://bookdown.org/rdpeng/rprogdatascience/},
  urldate = {2025-02-25},
  abstract = {The R programming language has become the de facto programming language for data science. Its flexibility, power, sophistication, and expressiveness have made it an invaluable tool for data scientists around the world. This book is about the fundamentals of R programming. You will get started with the basics of the language, learn how to manipulate datasets, how to write functions, and how to debug and optimize code. With the fundamentals provided in this book, you will have a solid foundation on which to build your data science toolbox.},
  langid = {english},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,programming languages,r (programming language),textbooks},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Peng - 2022 - R programming for data science.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Peng - 2022 - R programming for data science.pdf}
}

@article{perezgonzalez2015,
  title = {Fisher, {{Neyman-Pearson}} or {{NHST}}? {{A}} tutorial for teaching data testing},
  shorttitle = {Fisher, {{Neyman-Pearson}} or {{NHST}}?},
  author = {Perezgonzalez, Jose D.},
  date = {2015-03-02},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {6},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00223},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2015.00223/full},
  urldate = {2024-09-30},
  abstract = {Despite frequent calls for the overhaul of null hypothesis significance testing (NHST), this controversial procedure remains ubiquitous in behavioral, social and biomedical teaching and research. Little change seems possible once the procedure becomes well ingrained in the minds and current practice of researchers; thus, the optimal opportunity for such change is at the time the procedure is taught, be this at undergraduate or at postgraduate levels. This paper presents a tutorial for the teaching of data testing procedures, often referred to as hypothesis testing theories. The first procedure introduced is Fisher's approach to data testing—tests of significance; the second is Neyman-Pearson's approach—tests of acceptance; the final procedure is the incongruent combination of the previous two theories into the current approach—NSHT. For those researchers sticking with the latter, two compromise solutions on how to improve NHST conclude the tutorial.},
  langid = {english},
  keywords = {exact sciences,fisher,history,hypothesis tests,hypothetico–deductive method,neyman-pearson,null hypothesis significance testing,science,statistical education,teaching statistics,test of significance,test of statistical hypothesis},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Perezgonzalez - 2015 - Fisher, Neyman-Pearson or NHST A tutorial for teaching data testing.pdf}
}

@book{popper1979a,
  title = {Objective knowledge: {{An}} evolutionary approach},
  shorttitle = {Objective knowledge},
  author = {Popper, Karl Raimund},
  date = {1979},
  origdate = {1972},
  publisher = {Oxford University Press},
  location = {Oxford, UK},
  isbn = {0-19-875024-2},
  langid = {english},
  pagetotal = {395},
  keywords = {epistemology,ontology,philosophy,science,scientific methodology},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Popper - 1979 - Objective knowledge.pdf}
}

@software{positteama,
  title = {{{RStudio}}: {{Integrated}} development environment for {{R}}},
  author = {{Posit Team}},
  url = {http://www.posit.co},
  organization = {Posit Software},
  keywords = {computer science,data science,exact sciences,nosource,programming,r (programming language)}
}

@software{rcoreteama,
  title = {R: {{A}} language and environment for statistical computing},
  author = {{R Core Team}},
  location = {Vienna, Austria},
  url = {https://www.R-project.org},
  organization = {R Foundation for Statistical Computing},
  keywords = {computer science,data science,exact sciences,nosource,programming,r (programming language)}
}

@book{reis2022,
  title = {Fundamentals of data engineering: plan and build robust data systems},
  shorttitle = {Fundamentals of data engineering},
  author = {Reis, Joe and Housley, Matt},
  date = {2022},
  publisher = {O'Reilly},
  location = {Sebastopol, CA},
  isbn = {978-1-0981-0830-4},
  langid = {english},
  keywords = {data engineering,engineering,exact sciences,fundamentals of data engineering,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.pdf}
}

@article{rowley2007,
  title = {The wisdom hierarchy: representations of the {{DIKW}} hierarchy},
  shorttitle = {The wisdom hierarchy},
  author = {Rowley, Jennifer},
  date = {2007-04-01},
  journaltitle = {Journal of Information Science},
  volume = {33},
  number = {2},
  pages = {163--180},
  publisher = {SAGE Publications Ltd},
  issn = {0165-5515},
  doi = {10.1177/0165551506070706},
  url = {https://doi.org/10.1177/0165551506070706},
  urldate = {2024-06-10},
  abstract = {This paper revisits the data-information-knowledge-wisdom (DIKW) hierarchy by examining the articulation of the hierarchy in a number of widely read textbooks, and analysing their statements about the nature of data, information, knowledge, and wisdom. The hierarchy referred to variously as the ‘Knowledge Hierarchy’, the ‘Information Hierarchy’ and the ‘Knowledge Pyramid’ is one of the fundamental, widely recognized and ‘taken-for-granted’ models in the information and knowledge literatures. It is often quoted, or used implicitly, in definitions of data, information and knowledge in the information management, information systems and knowledge management literatures, but there has been limited direct discussion of the hierarchy. After revisiting Ackoff’s original articulation of the hierarchy, definitions of data, information, knowledge and wisdom as articulated in recent textbooks in information systems and knowledge management are reviewed and assessed, in pursuit of a consensus on definitions and transformation processes. This process brings to the surface the extent of agreement and dissent in relation to these definitions, and provides a basis for a discussion as to whether these articulations present an adequate distinction between data, information, and knowledge. Typically information is defined in terms of data, knowledge in terms of information, and wisdom in terms of knowledge, but there is less consensus in the description of the processes that transform elements lower in the hierarchy into those above them, leading to a lack of definitional clarity. In addition, there is limited reference to wisdom in these texts.},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Rowley - 2007 - The wisdom hierarchy.pdf}
}

@article{schucany2006,
  title = {Preliminary goodness-of-fit tests for normality do not validate the one-sample {{Student}} t},
  author = {Schucany, William R. and Ng, H. K. Tony},
  date = {2006-12-01},
  journaltitle = {Communications in Statistics - Theory and Methods},
  volume = {35},
  number = {12},
  pages = {2275--2286},
  publisher = {Taylor \& Francis Group},
  doi = {10.1080/03610920600853308},
  url = {https://www.tandfonline.com/doi/abs/10.1080/03610920600853308},
  urldate = {2024-09-29},
  abstract = {One of the most basic topics in many introductory statistical methods texts is inference for a population mean, μ. The primary tool for confidence intervals and tests is the Student t sampling dist...},
  langid = {english},
  keywords = {assumption checks,exact sciences,general linear models,normality tests,probability and statistics,statistical assumptions,tests}
}

@article{shatz2024,
  title = {Assumption-checking rather than (just) testing: {{The}} importance of visualization and effect size in statistical diagnostics},
  shorttitle = {Assumption-checking rather than (just) testing},
  author = {Shatz, Itamar},
  date = {2024-02-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {56},
  number = {2},
  pages = {826--845},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02072-x},
  url = {https://doi.org/10.3758/s13428-023-02072-x},
  urldate = {2024-09-29},
  abstract = {Statistical methods generally have assumptions (e.g., normality in linear regression models). Violations of these assumptions can cause various issues, like statistical errors and biased estimates, whose impact can range from inconsequential to critical. Accordingly, it is important to check these assumptions, but this is often done in a flawed way. Here, I first present a prevalent but problematic approach to diagnostics—testing assumptions using null hypothesis significance tests (e.g., the Shapiro–Wilk test of normality). Then, I consolidate and illustrate the issues with this approach, primarily using simulations. These issues include statistical errors (i.e., false positives, especially with large samples, and false negatives, especially with small samples), false binarity, limited descriptiveness, misinterpretation (e.g., of p-value as an~effect size), and potential testing failure due to unmet test assumptions. Finally, I synthesize the implications of these issues for statistical diagnostics, and provide practical recommendations for improving such diagnostics. Key recommendations include maintaining awareness of the issues with assumption tests (while recognizing they can be useful), using appropriate combinations of diagnostic methods (including visualization and effect sizes) while recognizing their limitations, and distinguishing between testing and checking assumptions. Additional recommendations include judging assumption violations as a complex spectrum (rather than a simplistic binary), using programmatic tools that increase replicability and decrease researcher degrees of freedom, and sharing the material and rationale involved in the diagnostics.},
  langid = {english},
  keywords = {assumption checks,exact sciences,general linear models,graphical methods,null hypothesis significance testing,probability and statistics,statistical assumptions,statistical diagnostics,visualization},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Shatz - 2024 - Assumption-checking rather than (just) testing- the importance of visualization and effect size in s.pdf}
}

@dataset{sisvanb,
  title = {Relatórios de acesso público},
  author = {{Sistema de Vigilância Alimentar e Nutricional}},
  url = {https://sisaps.saude.gov.br/sisvan/relatoriopublico/},
  urldate = {2023-03-19},
  langid = {brazilian},
  keywords = {brazil,citizen science,exact sciences,nosource,nutrition,nutritional status,open data,open science,probability and statistics,sisvan}
}

@article{student1908,
  title = {The probable error of a mean},
  author = {{Student}},
  date = {1908},
  journaltitle = {Biometrika},
  volume = {6},
  number = {1},
  eprint = {2331554},
  eprinttype = {jstor},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2331554},
  url = {https://www.jstor.org/stable/2331554},
  urldate = {2024-11-20},
  langid = {english},
  keywords = {classic publications,exact sciences,probability and statistics,t-test},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Student - 1908 - The probable error of a mean.pdf}
}

@article{turing1937,
  title = {On computable numbers, with an application to the entscheidungsproblem},
  author = {Turing, A. M.},
  date = {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  shortjournal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  issn = {00246115},
  doi = {10.1112/plms/s2-42.1.230},
  url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {computer science,exact sciences,historical publications,máquina computadora universal},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Turing - 1937 - On computable numbers, with an application to the entscheidungsproblem.pdf}
}

@book{vanderloo2018,
  title = {Statistical data cleaning with applications in {{R}}},
  author = {family=Loo, given=Mark, prefix=van der, useprefix=true and family=Jonge, given=Edwin, prefix=de, useprefix=false},
  date = {2018},
  publisher = {John Wiley \& Sons},
  location = {Hoboken, NJ},
  abstract = {A comprehensive guide to automated statistical data cleaning  The production of clean data is a complex and time-consuming process that requires both technical know-how and statistical expertise. Statistical Data Cleaning brings together a wide range of techniques for cleaning textual, numeric or categorical data. This book examines technical data cleaning methods relating to data representation and data structure. A prominent role is given to statistical data validation, data cleaning based on predefined restrictions, and data cleaning strategy. Key features: - Focuses on the automation of data cleaning methods, including both theory and applications written in R.   - Enables the reader to design data cleaning processes for either one-off analytical purposes or for setting up production systems that clean data on a regular basis.   - Explores statistical techniques for solving issues such as incompleteness, contradictions and outliers, integration of data cleaning components and quality monitoring.   - Supported by an accompanying website featuring data and R code. This book enables data scientists and statistical analysts working with data to deepen their understanding of data cleaning as well as to upgrade their practical data cleaning skills. It can also be used as material for a course in data cleaning and analyses.},
  isbn = {978-1-118-89714-0},
  langid = {english},
  pagetotal = {300},
  keywords = {computer science,data munging,data science,exact sciences,probability and statistics,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Jonge - 2018 - Statistical data cleaning with applications in R.pdf}
}

@thesis{vartanian2025a,
  type = {Corrected version (Pre-Release)},
  title = {Is latitude associated with chronotype?},
  author = {Vartanian, Daniel},
  date = {2025-01-13},
  institution = {University of São Paulo},
  location = {São Paulo, SP},
  doi = {10.17605/OSF.IO/YGKTS},
  url = {https://doi.org/10.17605/OSF.IO/YGKTS},
  urldate = {2025-01-14},
  langid = {english},
  pagetotal = {50},
  keywords = {biological sciences,chronobiology,chronotype,complexity science,entrainment,latitude},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Vartanian - 2025 - Is Latitude Associated with Chronotype.pdf}
}

@article{vonneumann1993a,
  title = {First draft of a report on the {{EDVAC}}},
  author = {family=Neumann, given=John, prefix=von, useprefix=true},
  date = {1993},
  journaltitle = {IEEE Annals of the History of Computing},
  volume = {15},
  number = {4},
  pages = {27--75},
  issn = {1934-1547},
  doi = {10.1109/85.238389},
  url = {https://ieeexplore.ieee.org/document/238389},
  urldate = {2025-02-24},
  abstract = {The first draft of a report on the EDVAC written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine (ACE) as the definitive source for understanding the nature and design of a general-purpose digital computer.},
  eventtitle = {{{IEEE Annals}} of the {{History}} of {{Computing}}},
  langid = {english},
  keywords = {computer science,electrical engineering,engines,exact sciences,extraordinary publications,forward contracts,history,laboratories,mathematics,pain,physics computing,proposals,statistics,stored-program concept},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/von Neumann - 1993 - First draft of a report on the EDVAC 2.pdf}
}

@book{vygotsky2012,
  title = {Thought and language},
  author = {Vygotsky, Lev Semyonovich},
  translator = {Hanfmann, Eugenia and Vakar, Gertruda and Kozulin, Alex},
  date = {2012},
  edition = {Rev. exp. ed.},
  publisher = {The MIT Press},
  location = {Cambridge, MA},
  isbn = {978-0-262-51771-3},
  langid = {english},
  pagetotal = {307},
  keywords = {cultural-historical theory,human sciences,learning,psychology,theory of mind,vygotsky},
  annotation = {Publicado originalmente em 1934.},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Vygotsky - 2012 - Thought and language.pdf}
}

@article{wasserstein2016,
  title = {The {{ASA}} statement on p-values: {{Context}}, process, and purpose},
  shorttitle = {The {{ASA}} statement on p-values},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  volume = {70},
  number = {2},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108},
  urldate = {2024-12-03},
  langid = {english},
  keywords = {consensus,exact sciences,p-value,p-value problem,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wasserstein and Lazar - 2016 - The ASA statement on p-values context, process, and purpose.pdf}
}

@article{wickham2014,
  title = {Tidy data},
  author = {Wickham, Hadley},
  date = {2014-09-12},
  journaltitle = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  url = {https://doi.org/10.18637/jss.v059.i10},
  urldate = {2024-06-08},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  langid = {english},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/XIK5UPP8/Wickham - 2014 - Tidy data.pdf}
}

@book{wickham2016a,
  title = {ggplot2: {{Elegant}} graphics for data analysis},
  shorttitle = {ggplot2},
  author = {Wickham, Hadley},
  namea = {Sievert, Carson},
  nameatype = {collaborator},
  date = {2016-06-16},
  series = {Use {{R}}!},
  edition = {2},
  publisher = {Springer},
  location = {Cham, Switzerland},
  doi = {10.1007/978-3-319-24277-4},
  url = {https://ggplot2-book.org},
  abstract = {This new edition to the classic book by ggplot2 creator Hadley Wickham highlights compatibility with knitr and RStudio. ggplot2 is a data visualization package for R that helps users create data graphics, including those that are multi-layered, with ease. With ggplot2, it's easy to:produce handsome, publication-quality plots with automatic legends created from the plot specificationsuperimpose multiple layers (points, lines, maps, tiles, box plots) from different data sources with automatically adjusted common scalesadd customizable smoothers that use powerful modeling capabilities of R, such as loess, linear models, generalized additive models, and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements and that can easily be applied to multiple plotsapproach a graph from a visual perspective, thinking about how each component of the data is represented on the final plotThis book will be useful to everyone who has struggled with displaying data in an informative and attractive way. Some basic knowledge of R is necessary (e.g., importing data into R).~ ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page.},
  isbn = {978-3-319-24277-4},
  langid = {english},
  pagetotal = {260},
  keywords = {computer science,data science,data visualization,ggplot2,grammar of graphics,probability and statistics,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2016 - ggplot2 Elegant graphics for data analysis.pdf}
}

@book{wickham2019,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  date = {2019-05-30},
  edition = {2},
  publisher = {CRC Press},
  location = {Boca Raton},
  url = {https://adv-r.hadley.nz},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimising your code. By reading this book, you will learn: - The difference between an object and its name, and why the distinction is important - The important vector data structures, how they fit together, and how you can pull them apart using subsetting - The fine details of functions and environments - The condition system, which powers messages, warnings, and errors - The powerful functional programming paradigm, which can replace many for loops - The three most important OO systems: S3, S4, and R6 - The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation - Effective debugging techniques that you can deploy, regardless of how your code is run - How to find and remove performance bottlenecks The second edition is a comprehensive update: - New foundational chapters: "Names and values," "Control flow," and "Conditions" - Comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them - Much deeper coverage of metaprogramming, including the new tidy evaluation framework - Use of new package like rlang (rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (purrr.tidyverse.org/) for functional programming - Use of color in code chunks and figures},
  isbn = {978-0-8153-8457-1},
  langid = {english},
  pagetotal = {588},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@online{wickham2023c,
  title = {The tidy tools manifesto},
  author = {Wickham, Hadley},
  date = {2023-02-23},
  url = {https://tidyverse.tidyverse.org/articles/manifesto.html},
  urldate = {2023-07-18},
  abstract = {tidyverse},
  langid = {english},
  organization = {Tidyverse},
  keywords = {data engineering,data science,engineering,guia de estilo,nosource,programming,r (programming language),software engineering}
}

@book{wickham2023d,
  title = {R packages},
  author = {Wickham, Hadley and Bryan, Jennifer},
  date = {2023},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r-pkgs.org},
  urldate = {2024-07-07},
  abstract = {Turn your R code into packages that others can easily install and use. With this fully updated edition, developers and data scientists will learn how to bundle reusable R functions, sample data, and documentation together by applying the package development philosophy used by the team that maintains the "tidyverse" suite of packages. In the process, you'll learn how to automate common development tasks using a set of R packages, including devtools, usethis, testthat, and roxygen2. Authors Hadley Wickham and Jennifer Bryan from Posit (formerly known as RStudio) help you create packages quickly, then teach you how to get better over time. You'll be able to focus on what you want your package to do as you progressively develop greater mastery of the structure of a package.},
  isbn = {978-1-0981-3494-5},
  langid = {english},
  pagetotal = {381},
  keywords = {data science,engineering,exact sciences,programming,r (programming language),r packages,software engineering},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.pdf}
}

@book{wickham2023e,
  title = {R for data science: {{Import}}, tidy, transform, visualize, and model data},
  shorttitle = {R for data science},
  author = {Wickham, Hadley and Çetinkaya-Rundel, Mine and Grolemund, Garrett},
  date = {2023-07-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r4ds.hadley.nz},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverseâ??a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way.},
  isbn = {978-1-4920-9740-2},
  langid = {american},
  pagetotal = {576},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.pdf}
}

@book{wickhamb,
  title = {The tidyverse style guide},
  author = {Wickham, Hadley},
  url = {https://style.tidyverse.org},
  urldate = {2023-07-17},
  langid = {english},
  keywords = {computer science,exact sciences,guia de estilo,nosource,r (programming language),standards}
}

@book{wickhamc,
  title = {Tidy design principles},
  author = {Wickham, Hadley},
  url = {https://design.tidyverse.org},
  langid = {english},
  keywords = {computer science,exact sciences,guia de estilo,nosource,r (programming language),standards}
}

@book{wilkinson2005,
  title = {The grammar of graphics},
  author = {Wilkinson, Leland},
  editor = {Chambers, J. and Hand, D. and Härdle, W.},
  editortype = {redactor},
  namea = {Wills, Graham and Rope, Dan and Norton, Andrew and Dubbs, Roger},
  nameatype = {collaborator},
  date = {2005},
  series = {Statistics and {{Computing}}},
  edition = {2},
  publisher = {Springer},
  location = {New York, NY},
  abstract = {Before writing the graphics for SYSTAT in the 1980’s, I began by teaching a seminar in statistical graphics and collecting as many different quantitative graphics as I could find. I was determined to produce a package that could draw every statistical graphic I had ever seen. The structure of the program was a collection of procedures named after the basic graph types they p- duced. The graphics code was roughly one and a half megabytes in size. In the early 1990’s, I redesigned the SYSTAT graphics package using - ject-based technology. I intended to produce a more comprehensive and - namic package. I accomplished this by embedding graphical elements in a tree structure. Rendering graphics was done by walking the tree and editing worked by adding and deleting nodes. The code size fell to under a megabyte. In the late 1990’s, I collaborated with Dan Rope at the Bureau of Labor Statistics and Dan Carr at George Mason University to produce a graphics p- duction library called GPL, this time in Java. Our goal was to develop graphics components. This book was nourished by that project. So far, the GPL code size is under half a megabyte.},
  isbn = {978-0-387-24544-7},
  langid = {english},
  pagetotal = {690},
  keywords = {computer science,data science,data visualization,design,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/TRDYALYB/Wilkinson - 2005 - The grammar of graphics.pdf}
}

@article{wilkinson2016,
  title = {The {{FAIR}} guiding principles for scientific data management and stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and Da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’T Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and Van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and Van Der Lei, Johan and Van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2023-07-17},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {data management,data science,exact sciences,open data,open science,princípios fair,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wilkinson - 2016 - The FAIR guiding principles for scientific data management and stewardship.pdf}
}

@video{zaidan2025,
  entrysubtype = {video},
  title = {How are microchips made?},
  editor = {Zaidan, George and Saini, Sajan},
  editortype = {director},
  date = {2025-02-25},
  publisher = {TED-Ed},
  url = {https://youtu.be/IkRXpFIRUl4?si=iQ7xQuFS6DZLuBY7},
  urldate = {2024-08-04},
  abstract = {Globally, we produce more than a trillion computer chips every year. Which means about 20 trillion transistors are built every second— and this process is done in fewer than 500 fabrication plants. How do we build so many tiny, intricately-connected devices, so incredibly fast? George Zaidan and Sajan Saini explore how photolithography helps build these devices and its environmental impact. Lesson by George Zaidan and Sajan Saini, directed by Kozmonot Animation Studio.},
  langid = {english},
  keywords = {computer science,fundamentals of computer science}
}
