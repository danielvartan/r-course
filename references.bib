@article{ackoff1989,
  title = {From data to wisdom},
  author = {Ackoff, Russell},
  date = {1989},
  journaltitle = {Journal of Applied Systems Analysis},
  volume = {16},
  pages = {3--9},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ackoff - 1989 - From data to wisdom.pdf}
}

@online{broman2013,
  title = {Data science is statistics},
  author = {Broman, Karl},
  date = {2013-04-05T13:12:35+00:00},
  url = {https://kbroman.wordpress.com/2013/04/05/data-science-is-statistics/},
  urldate = {2024-06-10},
  langid = {english},
  organization = {The stupidest thing...},
  keywords = {data science,exact sciences,probability and statistics}
}

@book{brookshear2020,
  title = {Computer science: {{An}} overview},
  shorttitle = {Computer science},
  author = {Brookshear, J. Glenn and Brylow, Dennis},
  date = {2020},
  edition = {13 ed., Global edition},
  publisher = {Pearson},
  location = {Harlow, UK},
  abstract = {Computer Science: An Overview, 13th edition, Global Edition, by J. Glenn Brookshear, and Dennis Brylow, is written for students from all backgrounds, giving you a bottom-up, concrete-to-abstract foundation in the subject. Its broad coverage encourages a practical and realistic understanding of computer science, covering all the major concepts. The book's broad background exposes beginning computer science students to the breadth of the subject they plan to major in and teaches students from other backgrounds how to relate to the technical society in which they live. Learn in a flexible way with independent chapters you can study in any order with full-colour design to help you engage with the information. The text also uses Python to provide programming tools for exploration and experimentation in your learning. This 13th edition has been corrected and updated in each chapter to refine your learning experience. With more than 1,000 questions and exercises, the book trains your thinking skills with useful chapter review problems and contains questions surrounding social issues to reinforce core concepts. This text is comprehensive and highly accessible, making it ideal for undergraduate studies in computer science.},
  isbn = {978-1-292-26344-1},
  langid = {english},
  pagetotal = {735},
  keywords = {computer science,exact sciences,fundamentals of computer science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Brookshear - 2020 - Computer science.pdf}
}

@article{cao2017,
  title = {Data science: a comprehensive overview},
  shorttitle = {Data science},
  author = {Cao, Longbing},
  date = {2017-06-29},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {3},
  pages = {43},
  issn = {0360-0300},
  doi = {10.1145/3076253},
  url = {https://dl.acm.org/doi/10.1145/3076253},
  urldate = {2024-06-10},
  abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/U4EIKQ7Q/Cao - 2017 - Data Science A Comprehensive Overview.pdf}
}

@book{coronel2019,
  title = {Database systems: design, implementation, and management},
  shorttitle = {Database systems},
  author = {Coronel, Carlos and Morris, Steven A.},
  date = {2019},
  edition = {13},
  publisher = {Cengage},
  location = {Boston, MA},
  isbn = {978-1-337-62790-0},
  langid = {english},
  pagetotal = {802},
  keywords = {computer science,databases,exact sciences,information science,modeling,relational databases},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Coronel - 2019 - Database systems.pdf}
}

@article{dhar2023,
  title = {Data science and prediction},
  author = {Dhar, Vasant},
  date = {2023-12-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {56},
  number = {12},
  pages = {64--73},
  issn = {0001-0782},
  doi = {10.1145/2500499},
  url = {https://dl.acm.org/doi/10.1145/2500499},
  urldate = {2024-06-10},
  abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
  langid = {english},
  keywords = {data science,exact sciences,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Dhar - 2023 - Data science and prediction.pdf}
}

@article{ellis2018,
  title = {How to share data for collaboration},
  author = {Ellis, Shannon E. and Leek, Jeffrey T.},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  number = {1},
  eprint = {32981941},
  eprinttype = {pmid},
  pages = {53--57},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375987},
  url = {https://doi.org/10.1080/00031305.2017.1375987},
  urldate = {2024-06-17},
  abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
  keywords = {data science,exact sciences,interdisciplinary fields,probability and statistics,tidy data},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ellis - 2018 - How to share data for collaboration.pdf}
}

@online{gavin2020,
  title = {Big data will be dead in 5 years},
  author = {Gavin, Lewis},
  date = {2020-10-20},
  url = {https://towardsdatascience.com/big-data-will-be-dead-in-5-years-ef4344269aef},
  urldate = {2024-06-10},
  abstract = {Everything will be big data, so it won't need a special name.},
  langid = {english},
  organization = {Towards Data Science},
  keywords = {big data,data science,exact sciences,probability and statistics}
}

@online{gofairinitiative,
  title = {{{GO FAIR}} initiative: make your data \& services {{FAIR}}},
  shorttitle = {{{GO FAIR}} initiative},
  author = {{GO FAIR initiative}},
  url = {https://www.go-fair.org/},
  urldate = {2024-06-10},
  abstract = {A bottom-up international approach for the practical implementation of the European Open Science Cloud (EOSC) as part of a global Internet of FAIR Data \& Services},
  langid = {american},
  organization = {GO FAIR},
  keywords = {data engineering,open data,open science,science}
}

@book{ifrah2001,
  title = {The universal history of computing: {{From}} the abacus to the quantum computer},
  shorttitle = {The universal history of computing},
  author = {Ifrah, Georges},
  translator = {Harding, E. F.},
  namea = {{Sophie Wood} and {Ian Monk} and {Elizabeth Clegg} and {Guido Waldman}},
  nameatype = {collaborator},
  date = {2001},
  origdate = {1994},
  publisher = {John Wiley \& Sons},
  location = {New York, NY},
  abstract = {Suppose every instrument could by command or by anticipation of need execute its function on its own; suppose that spindles could weave of their own accord, and plectra strike the strings of zithers by themselves; then craftsmen would have no need of hand-work, and masters have no need of slaves." Aristotle Called the Indiana Jones of arithmetic, Georges Ifrah embarked in 1974 on a ten-year quest to discover where numbers come from and what they say about us. His first book, the highly praised Universal History of Numbers, drew from this remarkable journey, presented the first complete account of the invention and evolution of numbers the world overand became an international bestseller. In The Universal History of Computing, Ifrah continues his exhilarating exploration into the fascinating world of numbers. In this fun, engaging but no less learned book, he traces the development of computing from the invention of the abacus to the creation of the binary system three centuries ago to the incredible conceptual, scientific, and technical achievements that made the first modern computers possible. He shows us how various cultures, scientists, and industries across the world struggled to break free of the tedious labor of mental calculation and, as a result, he reveals the evolution of the human mind. Evoking the excitement and joy that accompanied the grand mathematical undertakings throughout history, Ifrah takes us along as he revisits a multitude of cultures, from Roman times and the Chinese Common Era to twentieth-century England and America. We meet mathematicians, visionaries, philosophers, and scholars from every corner of the world and from every period of history. We witness the dead ends and regressions in the computers development, as well as the advances and illuminating discoveries. We learn about the births of the pocket calculator, the adding machine, the cash register, and even automata. We find out how the origins of the computer can be found in the European Renaissance, along with how World War II influenced the development of analytical calculation. And we explore such hot topics as numerical codes and the recent discovery of new kinds of number systems, such as "surreal" numbers. Adventurous and enthralling, The Universal History of Computing is an astonishing achievement that not only unravels the epic tale of computing, but also tells the compelling story of human intelligenceand how much farther we still have to go.},
  isbn = {0-471-39671-0},
  langid = {english},
  origlanguage = {fr},
  origlocation = {Paris},
  origpublisher = {Editions Robert Laffont},
  origtitle = {Histoire universelle des chiffres},
  pagetotal = {410},
  keywords = {computer science,exact sciences,history,human sciences},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Ifrah - 2001 - The universal history of computing From the abacus to the quantum computer.pdf}
}

@book{jonge2018a,
  title = {Statistical data cleaning with applications in {{R}}},
  author = {family=Jonge, given=Edwin, prefix=de, useprefix=false and family=Loo, given=Mark, prefix=van der, useprefix=false},
  date = {2018},
  publisher = {John Wiley \& Sons},
  location = {Hoboken, NJ},
  isbn = {978-1-118-89714-0 978-1-118-89713-3},
  pagetotal = {1},
  keywords = {data science,exact sciences,probability and statistics,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Jonge - 2018 - Statistical data cleaning with applications in R.pdf}
}

@book{landau,
  title = {The \{targets\} {{R}} package user manual},
  author = {Landau, Will},
  abstract = {Pipeline tools coordinate the pieces of computationally demanding analysis projects. The targets package is a Make-like pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.},
  langid = {english},
  keywords = {computer science,exact sciences,pipelines,programming,r (programming language),r packages}
}

@article{lohr2014,
  entrysubtype = {newspaper},
  title = {For big-data scientists, ‘{{Janitor}} work’ is key hurdle to insights},
  author = {Lohr, Steve},
  date = {2014-08-18},
  journaltitle = {The New York Times},
  location = {New York, NY},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html},
  urldate = {2024-06-10},
  abstract = {The analysis of giant data sets promises unique business insights, but much manual effort is still required to prepare the information for parsing.},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {data engineering,data science,exact sciences,probability and statistics}
}

@article{meyer2018,
  title = {Practical tips for ethical data sharing},
  author = {Meyer, Michelle N.},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {131--144},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245917747656},
  url = {https://doi.org/10.1177/2515245917747656},
  urldate = {2024-06-18},
  abstract = {This Tutorial provides practical dos and don’ts for sharing research data in ways that are effective, ethical, and compliant with the federal Common Rule. I first consider best practices for prospectively incorporating data-sharing plans into research, discussing what to say—and what not to say—in consent forms and institutional review board applications, tools for data de-identification and how to think about the risks of re-identification, and what to consider when selecting a data repository. Turning to data that have already been collected, I discuss the ethical and regulatory issues raised by sharing data when the consent form either was silent about data sharing or explicitly promised participants that the data would not be shared. Finally, I discuss ethical issues in sharing “public” data.},
  langid = {english},
  keywords = {ethics,open data,open science,science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Meyer - 2018 - Practical tips for ethical data sharing.pdf}
}

@book{reis2022,
  title = {Fundamentals of data engineering: plan and build robust data systems},
  shorttitle = {Fundamentals of data engineering},
  author = {Reis, Joe and Housley, Matt},
  date = {2022},
  publisher = {O'Reilly},
  location = {Sebastopol, CA},
  isbn = {978-1-0981-0830-4},
  langid = {english},
  keywords = {data engineering,engineering,exact sciences,fundamentals of data engineering,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Reis - 2022 - Fundamentals of data engineering.pdf}
}

@article{rowley2007,
  title = {The wisdom hierarchy: representations of the {{DIKW}} hierarchy},
  shorttitle = {The wisdom hierarchy},
  author = {Rowley, Jennifer},
  date = {2007-04-01},
  journaltitle = {Journal of Information Science},
  volume = {33},
  number = {2},
  pages = {163--180},
  publisher = {SAGE Publications Ltd},
  issn = {0165-5515},
  doi = {10.1177/0165551506070706},
  url = {https://doi.org/10.1177/0165551506070706},
  urldate = {2024-06-10},
  abstract = {This paper revisits the data-information-knowledge-wisdom (DIKW) hierarchy by examining the articulation of the hierarchy in a number of widely read textbooks, and analysing their statements about the nature of data, information, knowledge, and wisdom. The hierarchy referred to variously as the ‘Knowledge Hierarchy’, the ‘Information Hierarchy’ and the ‘Knowledge Pyramid’ is one of the fundamental, widely recognized and ‘taken-for-granted’ models in the information and knowledge literatures. It is often quoted, or used implicitly, in definitions of data, information and knowledge in the information management, information systems and knowledge management literatures, but there has been limited direct discussion of the hierarchy. After revisiting Ackoff’s original articulation of the hierarchy, definitions of data, information, knowledge and wisdom as articulated in recent textbooks in information systems and knowledge management are reviewed and assessed, in pursuit of a consensus on definitions and transformation processes. This process brings to the surface the extent of agreement and dissent in relation to these definitions, and provides a basis for a discussion as to whether these articulations present an adequate distinction between data, information, and knowledge. Typically information is defined in terms of data, knowledge in terms of information, and wisdom in terms of knowledge, but there is less consensus in the description of the processes that transform elements lower in the hierarchy into those above them, leading to a lack of definitional clarity. In addition, there is limited reference to wisdom in these texts.},
  langid = {english},
  keywords = {applied social sciences,dikw hierarchy,information science},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Rowley - 2007 - The wisdom hierarchy.pdf}
}

@article{turing1937,
  title = {On computable numbers, with an application to the entscheidungsproblem},
  author = {Turing, A. M.},
  date = {1937},
  journaltitle = {Proceedings of the London Mathematical Society},
  shortjournal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  issn = {00246115},
  doi = {10.1112/plms/s2-42.1.230},
  url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},
  urldate = {2024-08-04},
  langid = {english},
  keywords = {computer science,exact sciences,historical publications,máquina computadora universal},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Turing - 1937 - On computable numbers, with an application to the entscheidungsproblem.pdf}
}

@article{vonneumann1993a,
  title = {First draft of a report on the {{EDVAC}}},
  author = {family=Neumann, given=John, prefix=von, useprefix=true},
  date = {1993},
  journaltitle = {IEEE Annals of the History of Computing},
  volume = {15},
  number = {4},
  pages = {27--75},
  issn = {1934-1547},
  doi = {10.1109/85.238389},
  url = {https://ieeexplore.ieee.org/document/238389},
  urldate = {2025-02-24},
  abstract = {The first draft of a report on the EDVAC written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine (ACE) as the definitive source for understanding the nature and design of a general-purpose digital computer.},
  eventtitle = {{{IEEE Annals}} of the {{History}} of {{Computing}}},
  langid = {english},
  keywords = {computer science,electrical engineering,engines,exact sciences,extraordinary publications,forward contracts,history,laboratories,mathematics,pain,physics computing,proposals,statistics,stored-program concept},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/von Neumann - 1993 - First draft of a report on the EDVAC 2.pdf}
}

@article{wickham2014,
  title = {Tidy data},
  author = {Wickham, Hadley},
  date = {2014-09-12},
  journaltitle = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  url = {https://doi.org/10.18637/jss.v059.i10},
  urldate = {2024-06-08},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  langid = {english},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/data/storage/XIK5UPP8/Wickham - 2014 - Tidy data.pdf}
}

@book{wickham2019,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  date = {2019-05-30},
  edition = {2},
  publisher = {CRC Press},
  location = {Boca Raton},
  url = {https://adv-r.hadley.nz},
  abstract = {Advanced R helps you understand how R works at a fundamental level. It is designed for R programmers who want to deepen their understanding of the language, and programmers experienced in other languages who want to understand what makes R different and special. This book will teach you the foundations of R; three fundamental programming paradigms (functional, object-oriented, and metaprogramming); and powerful techniques for debugging and optimising your code. By reading this book, you will learn: - The difference between an object and its name, and why the distinction is important - The important vector data structures, how they fit together, and how you can pull them apart using subsetting - The fine details of functions and environments - The condition system, which powers messages, warnings, and errors - The powerful functional programming paradigm, which can replace many for loops - The three most important OO systems: S3, S4, and R6 - The tidy eval toolkit for metaprogramming, which allows you to manipulate code and control evaluation - Effective debugging techniques that you can deploy, regardless of how your code is run - How to find and remove performance bottlenecks The second edition is a comprehensive update: - New foundational chapters: "Names and values," "Control flow," and "Conditions" - Comprehensive coverage of object oriented programming with chapters on S3, S4, R6, and how to choose between them - Much deeper coverage of metaprogramming, including the new tidy evaluation framework - Use of new package like rlang (rlang.r-lib.org), which provides a clean interface to low-level operations, and purr (purrr.tidyverse.org/) for functional programming - Use of color in code chunks and figures},
  isbn = {978-0-8153-8457-1},
  langid = {english},
  pagetotal = {588},
  keywords = {computer science,data engineering,data science,exact sciences,probability and statistics,r (programming language),textbooks}
}

@book{wickham2023d,
  title = {R packages},
  author = {Wickham, Hadley and Bryan, Jennifer},
  date = {2023},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r-pkgs.org},
  urldate = {2024-07-07},
  abstract = {Turn your R code into packages that others can easily install and use. With this fully updated edition, developers and data scientists will learn how to bundle reusable R functions, sample data, and documentation together by applying the package development philosophy used by the team that maintains the "tidyverse" suite of packages. In the process, you'll learn how to automate common development tasks using a set of R packages, including devtools, usethis, testthat, and roxygen2. Authors Hadley Wickham and Jennifer Bryan from Posit (formerly known as RStudio) help you create packages quickly, then teach you how to get better over time. You'll be able to focus on what you want your package to do as you progressively develop greater mastery of the structure of a package.},
  isbn = {978-1-0981-3494-5},
  langid = {english},
  pagetotal = {381},
  keywords = {data science,engineering,exact sciences,programming,r (programming language),r packages,software engineering},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R packages.pdf}
}

@book{wickham2023e,
  title = {R for data science: {{Import}}, tidy, transform, visualize, and model data},
  shorttitle = {R for data science},
  author = {Wickham, Hadley and Çetinkaya-Rundel, Mine and Grolemund, Garrett},
  date = {2023-07-18},
  edition = {2},
  publisher = {O'Reilly Media},
  location = {Sebastopol, CA},
  url = {https://r4ds.hadley.nz},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverseâ??a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way.},
  isbn = {978-1-4920-9740-2},
  langid = {american},
  pagetotal = {576},
  keywords = {data engineering,data science,exact sciences,probability and statistics,programming,r (programming language)},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.epub;/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wickham - 2023 - R for data science.pdf}
}

@article{wilkinson2016,
  title = {The {{FAIR}} guiding principles for scientific data management and stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and Da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’T Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and Van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and Van Der Lei, Johan and Van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2023-07-17},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {data management,data science,exact sciences,open data,open science,princípios fair,probability and statistics},
  file = {/home/danielvartan/Insync/danvartan@gmail.com/Google Drive/Zotero/files/Wilkinson - 2016 - The FAIR guiding principles for scientific data management and stewardship.pdf}
}
