## Modeling with Tidyverse *versus* Modeling with base R {.smaller}

<!-- Teach only the tidymodels approach -->

The [tidyverse](https://www.tidyverse.org/) framework offers a structured and consistent approach to modeling in R through the [tidymodels](https://www.tidymodels.org/) ecosystem.

While many users still utilize base R functions for modeling, we will focus on the tidyverse approach due to its simplicity and readability.

::: {.notes}
:::

## The Tidymodels Framework {.smaller}

[![](images/tidymodels-hex-logo.png){style="width: 120px;"}](https://www.tidymodels.org/)

The `tidymodels` framework is a collection of packages for modeling and machine learning using [tidyverse](https://www.tidyverse.org/) principles.

::: {.notes}
:::

## The *p*-value Problem {.smaller}

::::: {.columns}
:::: {.column style="width: 40%; padding-top: 0px;"}
Large samples and sensitivity

Is a difference of $0.00001$ valid?

Statistical ritual *versus* Statistical thinking

::: {style="font-size: 0.5em; font-style: italic; padding-top: 15px;"}
Comparison of a 95% of confidence level ($\alpha = 0.05$) and an *n*-dependent *p-value* curve. The parameter $n_{\alpha}$ represents the minimum sample size to detect statistically significant differences among compared groups. The parameter $n_{\gamma}$ represents the convergence point of the *p-value* curve. When the *p-value* curve expresses practical differences, the area under the red curve ($A_{p(n)}$) is smaller than the area under the constant function $\alpha = 0.05$ ($A_{\alpha = 0.05}$) when it is evaluated between $0$ and $n_{\gamma}$.
:::
::::
:::: {.column style="width: 60%; padding-top: 100px;"}
![](images/gomez-de-mariscal-2021-figure-3.png){fig-align="center" style="width: 75%; padding-top: 0px;"}
::::
:::::

::: footer
(Chart reproduced from @mariscal2021[Figure 3])
:::

::: {.notes}
:::

## Power Analysis {.smaller}

::: {.notes}
:::

## Type I Errors ($\alpha$) {.smaller}

![](images/allison-horst-figure-39.png){fig-align="center" style="width: 90%; padding-top: 0px;"}

::: footer
(Artwork by [Allison Horst](https://twitter.com/allison_horst))
:::

::: {.notes}
:::

## Type II Errors ($\beta$) {.smaller}

![](images/allison-horst-figure-40.png){fig-align="center" style="width: 90%; padding-top: 30px;"}

::: footer
(Artwork by [Allison Horst](https://twitter.com/allison_horst))
:::

::: {.notes}
:::

## Cohen's Benchmark {.smaller}

:::: {.columns}
::: {.column style="width: 50%;"}
> [...] in many circumstances, all that is intended by "proving" the null hypothesis is that the ES [Effect Size] is not necessarily zero but **small enough to be negligible** [...]. (p. 461)
:::
::: {.column style="width: 50%;"}
![](images/jacob-cohen-figure-1.jpg){fig-align="center" style="width: 80%; padding-top: 25px;"}
:::
::::

::: footer
(Quotes reproduced from @cohen1988a. Photo by an unknown author.)
:::

::: {.notes}
:::

## Cohen's Benchmark {.smaller}

![](images/cohen-1992-table-1.png){fig-align="center" style="padding-top: 10px; padding-bottom: 25px;"}

::: footer
(Reproduced from @cohen1992)
:::

::: {.notes}
:::

## One-Sample t-Test: Hypothesis {.smaller}

$$
\begin{cases}
\text{H}_{0}: \mu_{A} = \mu_{B} \\
\text{H}_{a}: \mu_{A} \neq \mu_{B} \\
\end{cases}
$$

::: {.notes}
:::

## One-Sample t-Test: Hypothesis {.smaller}

```{r}
#| fig-align: center
#| code-fold: true
#| warning: false

library(brandr)
library(ggplot2)
library(palmerpenguins)
library(tidyr)

penguins |>
  dplyr::filter(species %in% c("Adelie", "Gentoo")) |>
  tidyr::drop_na(flipper_length_mm, species) |>
  ggplot(aes(x = species, y = flipper_length_mm, fill = species)) +
  geom_boxplot(outlier.color = brandr::get_brand_color("dark-red")) +
  geom_jitter(width = 0.2, alpha = 0.1) +
  brandr::scale_fill_brand_d(alpha = 0.7) +
  labs(x = "Species", y = "Flipper Length (mm)", fill = "Species")
```

::: {.notes}
:::

## One-Sample t-Test: `tidymodels` {.smaller}

```{r}
library(infer)
library(palmerpenguins)
```

```{r}
observed_statistic <-
  penguins |>
  dplyr::filter(species %in% c("Adelie", "Gentoo")) |>
  tidyr::drop_na(flipper_length_mm, species) |>
  specify(flipper_length_mm ~ species) |>
  hypothesize(null = "independence") |>
  calculate("t", order = c("Adelie", "Gentoo"))

observed_statistic
```

::: {.notes}
:::

## One-Sample t-Test: `tidymodels` {.smaller}

```{r}
null_dist <-
  penguins |>
  dplyr::filter(species %in% c("Adelie", "Gentoo")) |>
  tidyr::drop_na(flipper_length_mm, species) |>
  specify(flipper_length_mm ~ species) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate("t", order = c("Adelie", "Gentoo"))

null_dist
```

::: {.notes}
:::

## One-Sample t-Test: `tidymodels` {.smaller}

```{r}
null_dist |>
  visualize() +
  shade_p_value(obs_stat = observed_statistic, direction = "two-sided")
```

```{r}
null_dist |>
  get_p_value(obs_stat = observed_statistic, direction = "two-sided")
```

::: {.notes}
:::

## One-Sample t-Test: Base R {.smaller}

```{r}
adelie <-
  penguins |>
  dplyr::filter(species == "Adelie") |>
  dplyr::pull(flipper_length_mm)

gentoo <-
  penguins |>
  dplyr::filter(species == "Gentoo") |>
  dplyr::pull(flipper_length_mm)
```

::: {.notes}
:::

## One-Sample t-Test: Base R {.smaller}

```{r}
test <- stats::t.test(
  x = adelie,
  y = gentoo,
  alternative = "two.sided",
  mu = 0,
  paired = FALSE,
  var.equal = FALSE, # Welch's t-test,
  conf.level = 0.95
)

test
```

::: {.notes}
:::

## One-Sample t-Test: Effect Size {.smaller}

:::: {.columns}
::: {.column style="width: 47.5%;"}
```{r}
library(effectsize)

effect_size <-
  cohens_d(
    x = adelie,
    y = gentoo,
    mu = 0,
    adjust = TRUE, # Hedge's g
    ci = 0.95,
    alternative = "two.sided"
  )

effect_size
```
:::
::: {.column style="width: 5%;"}
:::
::: {.column style="width: 47.5%;"}
```{r}
effect_size |> interpret_hedges_g(rules = "cohen1988")
```
:::
::::

::: {.notes}
:::

## One-Sample t-Test: Power Analysis {.smaller}

```{r}
# A posteriori

library(pwrss)

pwrss.t.2means(
  mu1 = adelie |> mean(na.rm = TRUE),
  mu2 = gentoo |> mean(na.rm = TRUE),
  sd1 = adelie |> sd(na.rm = TRUE),
  sd2 = gentoo |> sd(na.rm = TRUE),
  paired = FALSE,
  n2 = gentoo |> length(),
  kappa = (adelie |> length()) / (gentoo |> length()),
  power = NULL,
  alpha = 0.05,
  welch.df = TRUE,
  alternative = "not equal"
)
```

::: {.notes}
:::

## One-Way ANOVA: Hypothesis {.smaller}

$$
\begin{cases}
\text{H}_{0}: \mu_{A} = \mu_{B} = \mu_{C} \\
\text{H}_{a}: \mu_{i} \neq \mu_{j}, \ \text{for some} \ i, j
\end{cases}
$$

::: {.notes}
:::

## One-Way ANOVA: Boxplots {.smaller}

```{r}
#| fig-align: center
#| code-fold: true
#| warning: false

library(brandr)
library(ggplot2)
library(palmerpenguins)
library(tidyr)

penguins |>
  drop_na(bill_length_mm, species) |>
  ggplot(aes(x = species, y = flipper_length_mm, fill = species)) +
  geom_boxplot(outlier.color = brandr::get_brand_color("dark-red")) +
  geom_jitter(width = 0.2, alpha = 0.1) +
  brandr::scale_fill_brand_d(alpha = 0.7) +
  labs(x = "Species", y = "Bill Length (mm)", fill = "Species")
```

::: {.notes}
:::

## One-Way ANOVA: Base R {.smaller}

```{r}
library(effectsize)
library(stats)
```

```{r}
fit <- aov(flipper_length_mm ~ species, data = penguins)

fit
```

::: {.notes}
:::

## One-Way ANOVA: Base R {.smaller}

```{r}
fit |> summary()
```

## One-Way ANOVA: Base R {.smaller}

```{r}
 fit |> TukeyHSD()
```

::: {.notes}
:::

## One-Way ANOVA: Effect Size {.smaller}

```{r}
effect_size <- fit |> eta_squared()

effect_size
```

```{r}
effect_size |> interpret_eta_squared(rules = "cohen1992")
```

::: {.notes}
:::

## One-Way ANOVA: Power Analysis {.smaller}

```{r}
# A posteriori

library(pwrss)

pwrss.f.ancova(
  eta2 = effect_size$Eta2,
  n.way = 1,
  n.levels = 3,
  n.covariates = 0,
  alpha = 0.05,
  n = penguins |> nrow(),
  power = NULL
)
```

::: {.notes}
:::

## One-Way ANCOVA: Hypothesis {.smaller}

The ANCOCA hypothesis is similar to the ANOVA hypothesis, but it includes a covariate.

$$
\begin{cases}
\text{H}_{0}: \mu_{A} = \mu_{B} = \mu_{C} \\
\text{H}_{a}: \mu_{i} \neq \mu_{j}, \ \text{for some} \ i, j
\end{cases}
$$

::: {.notes}
:::

## One-Way ANCOVA: Fit {.smaller}

```{r}
library(car)
library(effectsize)
library(multcomp)
library(stats)
```

```{r}
fit <- aov(flipper_length_mm ~ body_mass_g * species, data = penguins)

fit
```

::: {.notes}
:::

## One-Way ANCOVA: Fit Summary {.smaller}

```{r}
 fit |> Anova(type = "III")
```

::: {.notes}
:::

## One-Way ANCOVA: Tukey Test {.smaller}

```{r}
post_hoc <-
  fit |>
  glht(linfct = mcp(species = "Tukey"))

summary(post_hoc)
```

::: {.notes}
:::

## One-Way ANCOVA: Boxplots {.smaller}

```{r}
#| fig-align: center
#| code-fold: true
#| warning: false

library(brandr)
library(ggplot2)
library(palmerpenguins)
library(tidyr)

penguins |>
  drop_na(bill_length_mm, species) |>
  ggplot(aes(x = species, y = bill_length_mm, fill = species)) +
  geom_boxplot(outlier.color = brandr::get_brand_color("dark-red")) +
  geom_jitter(width = 0.2, alpha = 0.1) +
  brandr::scale_fill_brand_d(alpha = 0.7) +
  labs(x = "Species", y = "Bill Length (mm)", fill = "Species")
```

## One-Way ANCOVA: Effect Size {.smaller}

```{r}
library(effectsize)

effect_size <- fit |> eta_squared()

effect_size
```

```{r}
effect_size |> interpret_eta_squared(rules = "cohen1992")
```

::: {.notes}
:::

## One-Way ANCOVA: Power Analysis {.smaller}

```{r}
# A posteriori

library(pwrss)

pwrss.f.ancova(
  eta2 = effect_size$Eta2_partial[3],
  n.way = 1,
  n.levels = 3,
  n.covariates = 1,
  alpha = 0.05,
  n = penguins |> nrow(),
  power = NULL
)
```

::: {.notes}
:::

## Linear Regression: Hypothesis {.smaller}

To ensure practical significance, the adjusted $\text{R}^2$ of the model is analysed for its effect sizes considering a confidence interval of $95\%$. We are going to use @cohen1988a benchmark for interpretation.

A $\text{R}^2$ less than $\approx 0.0196$ is considered negligeble.

$$
\begin{cases}
\text{H}_{0}: \text{Adjusted} \ \text{R}^{2} \leq \text{MES} \quad \text{or} \quad \text{F-test is not significant} \ (\alpha \geq 0.05) \\
\text{H}_{a}: \text{Adjusted} \ \text{R}^{2} > \text{MES} \quad \text{and} \quad \text{F-test is significant} \ (\alpha < 0.05)
\end{cases}
$$

::: {.notes}
:::

## Linear Regression: Scatterplot 1 {.smaller}

```{r}
#| fig-align: center
#| code-fold: true

library(brandr)
library(ggplot2)
library(palmerpenguins)
library(tidyr)

penguins |>
  drop_na(flipper_length_mm, body_mass_g, species) |>
  ggplot(
    aes(
      x = flipper_length_mm,
      y = bill_length_mm,
      color = species,
      shape = species
    )
  ) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  labs(
    x = "Flipper Length (mm)",
    y = "Bill Length (mm)",
    color = "Species",
    shape = "Species"
  ) +
  brandr::scale_color_brand_d(alpha = 0.7)
```

::: {.notes}
:::

## Linear Regression: Scatterplot 2 {.smaller}

```{r}
#| fig-align: center
#| code-fold: true

library(brandr)
library(ggplot2)
library(palmerpenguins)
library(tidyr)

penguins |>
  drop_na(flipper_length_mm, body_mass_g, species) |>
  ggplot(
    aes(
      x = flipper_length_mm,
      y = bill_depth_mm,
      color = species,
      shape = species
    )
  ) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  labs(
    x = "Flipper Length (mm)",
    y = "Bill Depth (mm)",
    color = "Species",
    shape = "Species"
  ) +
  brandr::scale_color_brand_d(alpha = 0.7)
```

::: {.notes}
:::

## Linear Regression: Fit {.smaller}

```{r}
fit <- lm(
  formula = flipper_length_mm ~ bill_length_mm + bill_depth_mm,
  data = penguins
)

fit
```

::: {.notes}
:::

## Linear Regression: Fit Summary {.smaller}

```{r}
fit_sum <- fit |> summary()

fit_sum
```

::: {.notes}
:::

## Linear Regression: Effect Size {.smaller}

```{r}
library(effectsize)

fit_sum$r.squared |> interpret_r2(rules = "cohen1988")
```

::: {.notes}
:::

## Linear Regression: Power Analysis {.smaller}

```{r}
# A posteriori

library(pwrss)

pwrss.f.reg(
  r2 = fit_sum$r.squared,
  k = 2,
  n = penguins |> nrow(),
  power = NULL,
  alpha = 0.05
)
```

::: {.notes}
:::

## Diagnostics {.smaller}

::: {.notes}
:::

## Objective Assumption Tests {.smaller}

🚨 **Avoid Using!**

::: {.notes}
:::

## Normality Assumption Tests {.smaller}

![](images/allison-horst-figure-38.png){fig-align="center" style="width: 90%; padding-top: 10px; padding-bottom: 30px;"}

::: footer
(Artwork by [Allison Horst](https://twitter.com/allison_horst))
:::

::: {.notes}
:::

## How to Learn More {.smaller}

:::: {.columns}
::: {.column style="width: 47.5%;"}
[![](images/degroot-2012-book-cover.jpg){fig-align="center" style="width: 85%;"}](https://books.google.com.br/books/about/Probability_and_Statistics.html?id=4TlEPgAACAAJ&redir_esc=y)
:::
::: {.column style="width: 5%;"}
:::
::: {.column style="width: 47.5%;"}
[![](images/kuhn-2022-book-cover.jpg){fig-align="center" style="width: 83%;"}](https://www.tmwr.org/)
:::
::::

::: footer
(Book cover image from @degroot2012a and @kuhn2022)
:::

::: {.notes}
:::
